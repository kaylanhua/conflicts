{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using gpt to query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unclear if i even need this \n",
    "import pycountry\n",
    "import difflib\n",
    "\n",
    "def clean_country_name(name):\n",
    "    \"\"\"Find the best guess for a country name.\"\"\"\n",
    "    countries = [country.name for country in pycountry.countries]\n",
    "    closest_match = difflib.get_close_matches(name, countries, n=1)\n",
    "    return closest_match[0] if closest_match else None\n",
    "\n",
    "# Example usage\n",
    "clean_country_name(\"PRC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (1.2.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from googlesearch-python) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from googlesearch-python) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from newspaper3k) (10.1.0)\n",
      "Collecting PyYAML>=3.11 (from newspaper3k)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=3.6.0 (from newspaper3k)\n",
      "  Downloading lxml-4.9.3-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from newspaper3k) (2.31.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading tldextract-5.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kaylahuang/opt/anaconda3/envs/c-network/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading lxml-4.9.3-cp312-cp312-macosx_11_0_universal2.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=6858f819db2adfb72a76064e2cc524d4345e96ec949dfb7c6950726d0ac9cfc8\n",
      "  Stored in directory: /Users/kaylahuang/Library/Caches/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3339 sha256=c4c40920dd48d050b45f04909814ef515ed9b1d5f49ec00f9e2a4fb1550162d6\n",
      "  Stored in directory: /Users/kaylahuang/Library/Caches/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=935ef5e2f79cba6a1247184c18156b2c050d45247df66711cb1aa02cb7bf1011\n",
      "  Stored in directory: /Users/kaylahuang/Library/Caches/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=a9e9b985b75ecde2863271935f4da9c80b2e643b4c45bf0b859968b59a1177cc\n",
      "  Stored in directory: /Users/kaylahuang/Library/Caches/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, PyYAML, lxml, filelock, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed PyYAML-6.0.1 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 filelock-3.13.1 jieba3k-0.35.1 lxml-4.9.3 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python\n",
    "%pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://concernusa.org/news/timeline-south-sudan-history/', 'https://www.bbc.com/news/world-africa-14019202', 'https://www.aljazeera.com/news/2020/2/22/timeline-south-sudan-since-independence', 'https://www.aljazeera.com/news/2023/5/31/fighting-in-sudan-a-timeline-of-key-events', 'https://africacenter.org/wp-content/uploads/2019/12/Timeline-of-South-Sudan-Peace-Agreements-and-Violence-printable.pdf', 'https://www.rescuesouthsudan.org/sudan-timeline-of-events/', 'https://www.cfr.org/global-conflict-tracker/conflict/civil-war-south-sudan']\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "def google_search_links(query, num_results=10):\n",
    "    \"\"\"Return the top N links from a Google search.\"\"\"\n",
    "    return [link for i, link in enumerate(search(query, num_results))]\n",
    "\n",
    "# query = \"why did the conflict in south sudan happen\"\n",
    "query = \"timeline for the conflict in south sudan\"\n",
    "links = google_search_links(query, 5)\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def bs_parser(url):\n",
    "#     \"\"\"Return the text from a webpage.\"\"\"\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#     for script_or_style in soup(['script', 'style']):\n",
    "#         script_or_style.extract()\n",
    "    \n",
    "#     text = soup.get_text()\n",
    "#     body = soup.find('body')\n",
    "#     if body:\n",
    "#         text = body.get_text()\n",
    "    \n",
    "#     text = ' '.join(text.split())\n",
    "#     return text\n",
    "\n",
    "# for url in links:\n",
    "#     print(bs_parser(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "# Example URL\n",
    "url = 'https://concernusa.org/news/timeline-south-sudan-history/'\n",
    "\n",
    "def read_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article \n",
    "    # print(\"Title:\", article.title)\n",
    "    # print(\"Text:\", article.text)\n",
    "    # print(\"Publish Date:\", article.publish_date)\n",
    "    # article.nlp()\n",
    "    # print(\"Keywords:\", article.keywords)\n",
    "    # print(\"Summary:\", article.summary)\n",
    "\n",
    "with open('south_sudan.txt', 'w') as f:\n",
    "    for url in links:\n",
    "        article = read_article(url)\n",
    "        f.write(article.title)\n",
    "        f.write(article.text)\n",
    "        f.write('\\n\\n-----------------------------------\\n\\n')\n",
    "\n",
    "with open('south_sudan.txt', 'r') as f:\n",
    "    text_for_gpt = f.read()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the actual query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "openai.organization = \"org-raWgaVqCbuR9YlP1CIjclYHk\" # Harvard\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(True if openai.api_key else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "with open('res.txt', 'w') as txtfile:\n",
    "    completion = client.chat.completions.create(\n",
    "        # turbo (1106-preview) has 128k context window, about 300 pages of text\n",
    "        model=\"gpt-4-1106-preview\", # test with: gpt-3.5-turbo, run final: gpt-4-1106-preview\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \n",
    "                \"\"\"You are a conflict analyst. You are writing a report on the conflict in South Sudan. Someone has provided you with a few articles which outline the timeline of the conflict. You want to summarize the timeline in your report.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Here are the bodies of the articles: {text_for_gpt}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    gpt_summary = completion.choices[0].message.content\n",
    "    \n",
    "    txtfile.write(f\"{gpt_summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
