{
    "4b6555beef240120bacb699c7d2f7c8e806b5747": [
        {
            "paperId": "4b6555beef240120bacb699c7d2f7c8e806b5747",
            "url": "https://www.semanticscholar.org/paper/4b6555beef240120bacb699c7d2f7c8e806b5747",
            "title": "Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data",
            "abstract": "The most commonly used statistical models of civil war onset fail to correctly predict most occurrences of this rare event in out-of-sample data. Statistical methods for the analysis of binary data, such as logistic regression, even in their rare event and regularized forms, perform poorly at prediction. We compare the performance of Random Forests with three versions of logistic regression (classic logistic regression, Firth rare events logistic regression, and L 1-regularized logistic regression), and find that the algorithmic approach provides significantly more accurate predictions of civil war onset in out-of-sample data than any of the logistic regression models. The article discusses these results and the ways in which algorithmic statistical methods like Random Forests can be useful to more accurately predict rare events in conflict data.",
            "year": 2016,
            "authors": [
                {
                    "authorId": "102946994",
                    "name": "D. Muchlinski"
                },
                {
                    "authorId": "14459832",
                    "name": "David S. Siroky"
                },
                {
                    "authorId": "37395525",
                    "name": "Jingrui He"
                },
                {
                    "authorId": "36581389",
                    "name": "M. Kocher"
                }
            ]
        },
        [
            "a4d4053fa12ac75164fe2df0b20a4d3883c292c3",
            "da0a744bd257cc50a8778b4452b253bbe3c0b654",
            "3a8743b858ed557e3c722095ad4d4158e271e0bd",
            "17c7d2aada72fe59c8d282235b26089d526e44ee"
        ]
    ],
    "a4d4053fa12ac75164fe2df0b20a4d3883c292c3": [
        {
            "paperId": "a4d4053fa12ac75164fe2df0b20a4d3883c292c3",
            "url": "https://www.semanticscholar.org/paper/a4d4053fa12ac75164fe2df0b20a4d3883c292c3",
            "title": "edarf: Exploratory Data Analysis using Random Forests",
            "abstract": "This package contains functions useful for exploratory data analysis using random forests, which can be fit using the randomForest, randomForestSRC, or party packages (Liaw and Wiener 2002; Ishwaran and Kogalur 2013; Hothorn, Hornik, and Zeileis 2006). These functions can compute the partial dependence of covariates (individually or in combination) on the fitted forests\u2019 predictions, the permutation importance of covariates, as well as the distance between data points according to the fitted model.",
            "year": 2016,
            "authors": [
                {
                    "authorId": "119300401",
                    "name": "Z. Jones"
                },
                {
                    "authorId": "39083912",
                    "name": "Fridolin Linder"
                }
            ]
        },
        [
            "18c332b0bad296f8722677d2d63092bf02e0c210",
            "25badc676197a70aaf9911865eb03469e402ba57"
        ]
    ],
    "da0a744bd257cc50a8778b4452b253bbe3c0b654": [
        {
            "paperId": "da0a744bd257cc50a8778b4452b253bbe3c0b654",
            "url": "https://www.semanticscholar.org/paper/da0a744bd257cc50a8778b4452b253bbe3c0b654",
            "title": "Using Random Forests and Simulated Annealing to Predict Probabilities of Election to the Baseball Hall of Fame",
            "abstract": "A popular topic of argument among baseball fans is the prospective Hall of Fame status of current and recently retired players. A player's probability of enshrinement is likely to be affected by a large number of different variables, and can be approached by machine learning methods. In particular, I consider the use of random forests for this purpose. A random forest may be considered a black-box method for predicting the probability of Hall of Fame induction, but a number of parameters must be chosen before the forest can be grown. These parameters include fundamental aspects of the nuts and bolts of the construction of the trees that make up the forest, as well as choices among possible predictor variables. For example, one predictor that may be considered is a measure of the player's having seasons with many home runs hit, and there are multiple competing ways of measuring this. Furthermore, certain deterministic methods of searching the parameter space are partially undermined by the randomness underlying the forest's construction and the fact that, by sheer luck, two forests constructed with the same parameters may have differing qualities of fit. Using simulated annealing, I move through the parameter space in a stochastic fashion, trying many forests and sometimes moving toward a set of parameters even though its fit is apparently not as good as preceding ones. Since probabilities defined based on the votes of terminal nodes of a random forest for classification tend to be too moderate, the results of each forest considered are fed into a logistic regression to produce final probability estimates. From among four simulated annealing runs, the forest with the smallest mean squared error was selected, and analysis of the forests near it in the simulated annealing run indicate that its selection was probably not due to extraordinary \"luck.\" Predictions performed using the out-of-bag samples correctly identify 75% of Baseball Writers Association of America Hall of Fame selections, while misclassifying only 1% of non-selections. Results indicate a smaller mean squared error than a previous neural network approach, although the large number of forests tried and discarded raised concerns about overfitting in this case.",
            "year": 2010,
            "authors": [
                {
                    "authorId": "34945185",
                    "name": "M. Freiman"
                }
            ]
        },
        [
            "2d1e0a5d5309354d4948fa88b01232e59eee94dc"
        ]
    ],
    "3a8743b858ed557e3c722095ad4d4158e271e0bd": [
        {
            "paperId": "3a8743b858ed557e3c722095ad4d4158e271e0bd",
            "url": "https://www.semanticscholar.org/paper/3a8743b858ed557e3c722095ad4d4158e271e0bd",
            "title": "Navigating Random Forests and related advances in algorithmic modeling",
            "abstract": ": This article addresses current methodological research on non-parametric Random Forests. It provides a brief intellectual history of Random Forests that covers CART, boosting and bagging methods. It then introduces the primary methods by which researchers can visualize results, the relationships between covariates and responses, and the out-of-bag test set error. In addition, the article considers current research on universal consistency and importance tests in Random Forests. Finally, several uses for Random Forests are discussed, and available software is identi\ufb01ed.",
            "year": 2009,
            "authors": [
                {
                    "authorId": "14459832",
                    "name": "David S. Siroky"
                }
            ]
        },
        [
            "34122df837465c1f2025fabc3209964e7a3a205b",
            "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "70c87b478d421a54e2a712861fd23e2e45265062"
        ]
    ],
    "17c7d2aada72fe59c8d282235b26089d526e44ee": [
        {
            "paperId": "17c7d2aada72fe59c8d282235b26089d526e44ee",
            "url": "https://www.semanticscholar.org/paper/17c7d2aada72fe59c8d282235b26089d526e44ee",
            "title": "Conflict Prediction via Machine Learning: Addressing the Rare Events Problem with Bagging",
            "abstract": "Machine learning deals with the development of algorithms for classification and prediction. However, these algorithms have only in rare cases been used in political science. This poster demonstrates the application of state-of-the-art machine learning techniques to the prediction of conflict. In order to address the rare events problem, I use an ensemble of classifiers built on subsets of the training data. These subsets include all positive cases, and a random selection of negative ones. Although I focus primarily on decision tree learning, the proposed method can be used in conjunction with different classification algorithms in order to improve the prediction of conflict onset.",
            "year": 2008,
            "authors": [
                {
                    "authorId": "3026227",
                    "name": "Nils B. Weidmann"
                }
            ]
        },
        []
    ],
    "18c332b0bad296f8722677d2d63092bf02e0c210": [
        {
            "paperId": "18c332b0bad296f8722677d2d63092bf02e0c210",
            "url": "https://www.semanticscholar.org/paper/18c332b0bad296f8722677d2d63092bf02e0c210",
            "title": "Is There More Violence in the Middle?",
            "abstract": "Is there more violence in the middle? Over 100 studies have analyzed whether violent outcomes such as civil war, terrorism, and repression are more common in regimes that are neither full autocracies nor full democracies, yet findings are inconclusive. While this hypothesis is ultimately about functional form, existing work uses models in which a particular functional form is assumed. Existing work also uses arbitrary operationalizations of \u201cthe middle.\u201d This article aims to resolve the empirical uncertainty about this relationship by using a research design that overcomes the limitations of existing work. We use a random forest-like ensemble of multivariate regression and classification trees to predict multiple forms of conflict. Our results indicate the specific conditions under which there is or is not more violence in the middle. We find the most consistent support for the hypothesis with respect to minor civil conflict and no support with respect to repression. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network, at: https://doi.org/10.7910/DVN/LNUYXZ. What is the relationship between regime type and political violence? Are certain forms of conflict more likely in democracies or in autocracies? A series of influential studies has suggested this relationship is curvilinear, with violence most likely in regimes in the middle range\u2014often referred to as anocracies\u2014that are neither fully autocratic nor fully democratic (Eck and Hultman 2007; Fein 1995; Hegre et al. 2001). We refer to these arguments, collectively, as the More Violence in the Middle Hypothesis (or MVM Hypothesis). Despite decades of research, the extent to which such theories are empirically supported is unclear. While some early studies found that civil wars are most likely in anocracies (Hegre et al. 2001), others did not (Sambanis 2001). The debate may have appeared resolved when Vreeland (2008) showed that correcting for the extent to which measures of democracy might include indicators of violence results in no support for the MVM Hypothesis, but since then some have used his measure and found support for the hypothesis (Gleditsch and Ruggeri 2010), whereas others have confirmed his result (Peic and Reiter 2011). Likewise, some find evidence that terrorism is most Zachary M. Jones is Postdoctoral Fellow, eScience Institute, Campus Box 351570, University of Washington, Seattle, WA 98195-1570 (zmj@zmjones.com). Yonatan Lupu is Assistant Professor, Department of Political Science, George Washington University, Monroe Hall, Room 417, 2115 G Street, NW, Washington, DC 20052 (ylupu@gwu.edu). For comments on previous drafts, we thank Erica Chenoweth, Christian Davenport, Scott Gates, Will Moore, and Jake Shapiro, as well as participants in workshops at New York University, Uppsala University, and George Washington University. For research assistance, we thank Jack Hasler, Bryce Loidolt, and Steven Schaaf. common in anocracies (Wade and Reiter 2007), whereas others do not (Chenoweth 2010). With respect to repression, Davenport and Armstrong (2004) arguably settled the question by using more appropriate methods for testing this hypothesis than the bulk of the literature and finding no support for the MVM Hypothesis, but some recent work continues to find support for it (Mitchell, Ring, and Spellman 2013). The purpose of this article is to reduce the empirical uncertainty about the MVM Hypothesis and describe the conditions under which it does or does not hold. Although existing work has made significant progress, the methods used to date have several consequential limitations. The MVM Hypothesis is a prediction about the functional form of the relationship between regime type and conflict, yet almost all existing tests of the MVM Hypothesis have been conducted using models that assume a particular functional form and then test whether the data allow us to reject a simpler possible relationship between regime type and conflict, such as a monotonic relationship. While such tools can allow us to reject a monotonic relationship, they are not well suited for understanding the more complex ways in which regime type American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 652\u2013667 C \u00a92018, Midwest Political Science Association DOI: 10.1111/ajps.12373",
            "year": 2018,
            "authors": [
                {
                    "authorId": "119300401",
                    "name": "Z. Jones"
                },
                {
                    "authorId": "51169219",
                    "name": "Y. Lupu"
                }
            ]
        },
        [
            "a4d4053fa12ac75164fe2df0b20a4d3883c292c3",
            "873ee91bd87b4ec296db58618c891a39608e7fce",
            "4b6555beef240120bacb699c7d2f7c8e806b5747"
        ]
    ],
    "25badc676197a70aaf9911865eb03469e402ba57": [
        {
            "paperId": "25badc676197a70aaf9911865eb03469e402ba57",
            "url": "https://www.semanticscholar.org/paper/25badc676197a70aaf9911865eb03469e402ba57",
            "title": "Machine learning - a probabilistic perspective",
            "abstract": "Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.",
            "year": 2012,
            "authors": [
                {
                    "authorId": "2056417995",
                    "name": "Kevin P. Murphy"
                }
            ]
        },
        [
            "03230c3c83dbb013c3e610702c6f650307f0ce5c",
            "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "ed3a2b1d571a8188f41341f9b5c675a7682792b0",
            "878783964ab23c97052ea82685368099d85c500d",
            "f4ba954b0412773d047dc41231c733de0c1f4926",
            "3fc32c3ebbfa9de3168ae1c175bda839c9b44c21",
            "2d1e0a5d5309354d4948fa88b01232e59eee94dc",
            "e0999dc17b35c0d893974f03d98293f71f27698b",
            "d9c8b4f4a14b1568956f2a618c86728c6ce67390",
            "3d3c82d8989b0053da3fc7b0598cb72457442fd1",
            "a6b80923dd30f920234a1ac965bea768062882f8",
            "83644e8e95e1a2034b32d363bc6865bc2e23f8c5",
            "a5b40921ae03cc6fa34f532173a23c321c9ad3f7"
        ]
    ],
    "2d1e0a5d5309354d4948fa88b01232e59eee94dc": [
        {
            "paperId": "2d1e0a5d5309354d4948fa88b01232e59eee94dc",
            "url": "https://www.semanticscholar.org/paper/2d1e0a5d5309354d4948fa88b01232e59eee94dc",
            "title": "Bayesian CART Model Search",
            "abstract": "Abstract In this article we put forward a Bayesian approach for finding classification and regression tree (CART) models. The two basic components of this approach consist of prior specification and stochastic search. The basic idea is to have the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models. As the search proceeds, such models can then be selected with a variety of criteria, such as posterior probability, marginal likelihood, residual sum of squares or misclassification rates. Examples are used to illustrate the potential superiority of this approach over alternative methods.",
            "year": 1998,
            "authors": [
                {
                    "authorId": "1960946",
                    "name": "H. Chipman"
                },
                {
                    "authorId": "2347035",
                    "name": "E. George"
                },
                {
                    "authorId": "2840298",
                    "name": "R. McCulloch"
                }
            ]
        },
        []
    ],
    "34122df837465c1f2025fabc3209964e7a3a205b": [
        {
            "paperId": "34122df837465c1f2025fabc3209964e7a3a205b",
            "url": "https://www.semanticscholar.org/paper/34122df837465c1f2025fabc3209964e7a3a205b",
            "title": "Random Forests and Adaptive Nearest Neighbors",
            "abstract": "In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-PNNs) and show that random forests can be viewed as adaptively weighted k-PNN methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-PNNs in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-PNNs of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results.",
            "year": 2006,
            "authors": [
                {
                    "authorId": "2108066200",
                    "name": "Yi Lin"
                },
                {
                    "authorId": "3304956",
                    "name": "Yongho Jeon"
                }
            ]
        },
        []
    ],
    "807c1f19047f96083e13614f7ce20f2ac98c239a": [
        {
            "paperId": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "url": "https://www.semanticscholar.org/paper/807c1f19047f96083e13614f7ce20f2ac98c239a",
            "title": "C4.5: Programs for Machine Learning",
            "abstract": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.",
            "year": 1992,
            "authors": [
                {
                    "authorId": "145341779",
                    "name": "J. R. Quinlan"
                }
            ]
        },
        []
    ],
    "70c87b478d421a54e2a712861fd23e2e45265062": [
        {
            "paperId": "70c87b478d421a54e2a712861fd23e2e45265062",
            "url": "https://www.semanticscholar.org/paper/70c87b478d421a54e2a712861fd23e2e45265062",
            "title": "Maximum Likelihood Regression Trees",
            "abstract": "We put forward a new method of growing regression trees via maximum likelihood. It inherits the CART (Brieman et al., 1984) backward fitting idea. However, standard likelihood based methods such as model selection criteria and likelihood ratio tests are naturally incorporated into each stage of the tree procedure. Compared with other least squared tree methods, maximum likelihood regression trees (MLRT) reject the use of many ad hoc approaches and rely on more established methods; they have easy extension to handle data involving other types of responses; in addition, simulation study shows that MLRT tends to provide more accurate tree size selection than CART. The analysis of the 1992 baseball salary data is given as an illustration.",
            "year": null,
            "authors": [
                {
                    "authorId": "2257347428",
                    "name": "Xiaogang Su"
                }
            ]
        },
        [
            "2b461250c014b460e7c97b6138a3ee811f198f43"
        ]
    ],
    "873ee91bd87b4ec296db58618c891a39608e7fce": [
        {
            "paperId": "873ee91bd87b4ec296db58618c891a39608e7fce",
            "url": "https://www.semanticscholar.org/paper/873ee91bd87b4ec296db58618c891a39608e7fce",
            "title": "A Clash of Generations? Youth Bulges and Political Violence",
            "abstract": "It has frequently been suggested that exceptionally large youth cohorts, the so-called \u201cyouth bulges,\u201d make countries more susceptible to political violence. Within two prominent theoretical frameworks in the study of civil war, youth bulges are argued to potentially increase both opportunities and motives for political violence. This claim is empirically tested in a time-series cross-national statistical model for internal armed conflict for the period 1950\u20132000, and for event data for terrorism and rioting for the years 1984\u20131995. The expectation that youth bulges should increase the risk of political violence receives robust support for all three forms of violence. The results are consistent both with an expectation that youth bulges provide greater opportunities for violence through the abundant supply of youths with low opportunity costs, and with an expectation that stronger motives for violence may arise as youth bulges are more likely to experience institutional crowding, in particular unemployment. Some contextual factors have been suggested to potentially enhance the effect of youth bulges. In an empirical test of these propositions, the study suggests that youth bulges are particularly associated with an increasing risk of internal armed conflict in starkly autocratic regimes, but a similar effect is also found for highly democratic countries. The interaction of youth bulges with economic decline and expansion in higher education appear to increase the risk of terrorism but not of rioting. Recent studies in economic demography find that when fertility is sharply decreasing, causing lower dependency ratios, large youth cohorts entering the labor market may lead to economic boosts. This study finds some empirical evidence complementing these results, indicating that the effect of youth bulges on political violence may decline along with reduced dependency ratios.",
            "year": 2006,
            "authors": [
                {
                    "authorId": "6662929",
                    "name": "H. Urdal"
                }
            ]
        },
        [
            "2fcef1a302a3b3bda96bc3574ac1f7dc79c2ab48"
        ]
    ],
    "03230c3c83dbb013c3e610702c6f650307f0ce5c": [
        {
            "paperId": "03230c3c83dbb013c3e610702c6f650307f0ce5c",
            "url": "https://www.semanticscholar.org/paper/03230c3c83dbb013c3e610702c6f650307f0ce5c",
            "title": "A modern Bayesian look at the multi-armed bandit",
            "abstract": "A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are \u2018optimal\u2019 in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright \u00a9 2010 John Wiley & Sons, Ltd.",
            "year": 2010,
            "authors": [
                {
                    "authorId": "3124107",
                    "name": "S. L. Scott"
                }
            ]
        },
        [
            "4af9104d10a244a93ff08773d14dc3a3f2491329"
        ]
    ],
    "b71ac1e9fb49420d13e084ac67254a0bbd40f83f": [
        {
            "paperId": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "url": "https://www.semanticscholar.org/paper/b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).",
            "year": 2010,
            "authors": [
                {
                    "authorId": "3119801",
                    "name": "Xavier Glorot"
                },
                {
                    "authorId": "1751762",
                    "name": "Yoshua Bengio"
                }
            ]
        },
        []
    ],
    "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd": [
        {
            "paperId": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "url": "https://www.semanticscholar.org/paper/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
            "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.",
            "year": 2010,
            "authors": [
                {
                    "authorId": "145467703",
                    "name": "Pascal Vincent"
                },
                {
                    "authorId": "1777528",
                    "name": "H. Larochelle"
                },
                {
                    "authorId": "2453026",
                    "name": "Isabelle Lajoie"
                },
                {
                    "authorId": "1751762",
                    "name": "Yoshua Bengio"
                },
                {
                    "authorId": "1798462",
                    "name": "Pierre-Antoine Manzagol"
                }
            ]
        },
        []
    ],
    "ed3a2b1d571a8188f41341f9b5c675a7682792b0": [
        {
            "paperId": "ed3a2b1d571a8188f41341f9b5c675a7682792b0",
            "url": "https://www.semanticscholar.org/paper/ed3a2b1d571a8188f41341f9b5c675a7682792b0",
            "title": "BOOSTING ALGORITHMS: REGULARIZATION, PREDICTION AND MODEL FITTING",
            "abstract": "We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.",
            "year": 2007,
            "authors": [
                {
                    "authorId": "152235246",
                    "name": "Peter Buhlmann"
                },
                {
                    "authorId": "1970375",
                    "name": "T. Hothorn"
                }
            ]
        },
        [
            "f11261bd43487fe8f8083ff618d23f271ae38803",
            "be454e68b34b9a5ce784e64d420b24c4e454ff9b"
        ]
    ],
    "878783964ab23c97052ea82685368099d85c500d": [
        {
            "paperId": "878783964ab23c97052ea82685368099d85c500d",
            "url": "https://www.semanticscholar.org/paper/878783964ab23c97052ea82685368099d85c500d",
            "title": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation",
            "abstract": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.",
            "year": 2002,
            "authors": [
                {
                    "authorId": "145804005",
                    "name": "Robert Malouf"
                }
            ]
        },
        [
            "f4ba954b0412773d047dc41231c733de0c1f4926",
            "8f3899c8726d8316dabb2d5152be73a2a572b371"
        ]
    ],
    "f4ba954b0412773d047dc41231c733de0c1f4926": [
        {
            "paperId": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "url": "https://www.semanticscholar.org/paper/f4ba954b0412773d047dc41231c733de0c1f4926",
            "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
            "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
            "year": 2001,
            "authors": [
                {
                    "authorId": "1739581",
                    "name": "J. Lafferty"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "113414328",
                    "name": "Fernando Pereira"
                }
            ]
        },
        []
    ],
    "3fc32c3ebbfa9de3168ae1c175bda839c9b44c21": [
        {
            "paperId": "3fc32c3ebbfa9de3168ae1c175bda839c9b44c21",
            "url": "https://www.semanticscholar.org/paper/3fc32c3ebbfa9de3168ae1c175bda839c9b44c21",
            "title": "Deterministically annealed design of hidden Markov model speech recognizers",
            "abstract": "Many conventional speech recognition systems are based on the use of hidden Markov models (HMM) within the context of discriminant-based pattern classification. While the speech recognition objective is a low rate of misclassification, HMM design has been traditionally approached via maximum likelihood (ML) modeling which is, in general, mismatched with the minimum error objective and hence suboptimal. Direct minimization of the error rate is difficult because of the complex nature of the cost surface, and has only been addressed previously by discriminative design methods such as generalized probabilistic descent (GPD). While existing discriminative methods offer significant benefits, they commonly rely on local optimization via gradient descent whose performance suffers from the prevalence of shallow local minima. As an alternative, we propose the deterministic annealing (DA) design method that directly minimizes the error rate while avoiding many poor local minima of the cost. The DA is derived from fundamental principles of statistical physics and information theory. In DA, the HMM classifier's decision is randomized and its expected error rate is minimized subject to a constraint on the level of randomness which is measured by the Shannon entropy. The entropy constraint is gradually relaxed, leading in the limit of zero entropy to the design of regular nonrandom HMM classifiers. An efficient forward-backward algorithm is proposed for the DA method. Experiments on synthetic data and on a simplified recognizer for isolated English letters demonstrate that the DA design method can improve recognition error rates over both ML and GPD methods.",
            "year": 2001,
            "authors": [
                {
                    "authorId": "143672627",
                    "name": "A. Rao"
                },
                {
                    "authorId": "143629177",
                    "name": "K. Rose"
                }
            ]
        },
        [
            "eb4d687f1a3a2aac529ceac4a67f2290cc380317"
        ]
    ],
    "e0999dc17b35c0d893974f03d98293f71f27698b": [
        {
            "paperId": "e0999dc17b35c0d893974f03d98293f71f27698b",
            "url": "https://www.semanticscholar.org/paper/e0999dc17b35c0d893974f03d98293f71f27698b",
            "title": "Probabilistic Independence Networks for Hidden Markov Probability Models",
            "abstract": "Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.",
            "year": 1997,
            "authors": [
                {
                    "authorId": "50860274",
                    "name": "Padhraic Smyth"
                },
                {
                    "authorId": "48099028",
                    "name": "D. Heckerman"
                },
                {
                    "authorId": "1694621",
                    "name": "Michael I. Jordan"
                }
            ]
        },
        []
    ],
    "d9c8b4f4a14b1568956f2a618c86728c6ce67390": [
        {
            "paperId": "d9c8b4f4a14b1568956f2a618c86728c6ce67390",
            "url": "https://www.semanticscholar.org/paper/d9c8b4f4a14b1568956f2a618c86728c6ce67390",
            "title": "Diffusion of Context and Credit Information in Markovian Models",
            "abstract": "This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.",
            "year": 1995,
            "authors": [
                {
                    "authorId": "1751762",
                    "name": "Yoshua Bengio"
                },
                {
                    "authorId": "1688235",
                    "name": "P. Frasconi"
                }
            ]
        },
        []
    ],
    "3d3c82d8989b0053da3fc7b0598cb72457442fd1": [
        {
            "paperId": "3d3c82d8989b0053da3fc7b0598cb72457442fd1",
            "url": "https://www.semanticscholar.org/paper/3d3c82d8989b0053da3fc7b0598cb72457442fd1",
            "title": "Introduction to probability models",
            "abstract": "Ross's classic bestseller, Introduction to Probability Models, has been used extensively by professionals and as the primary text for a first undergraduate course in applied probability. It provides an introduction to elementary probability theory and stochastic processes, and shows how probability theory can be applied to the study of phenomena in fields such as engineering, computer science, management science, the physical and social sciences, and operations research. With the addition of several new sections relating to actuaries, this text is highly recommended by the Society of Actuaries. A new section (3.7) on COMPOUND RANDOM VARIABLES, that can be used to establish a recursive formula for computing probability mass functions for a variety of common compounding distributions. A new section (4.11) on HIDDDEN MARKOV CHAINS, including the forward and backward approaches for computing the joint probability mass function of the signals, as well as the Viterbi algorithm for determining the most likely sequence of states. Simplified Approach for Analyzing Nonhomogeneous Poisson processes Additional results on queues relating to the (a) conditional distribution of the number found by an M/M/1 arrival who spends a time t in the system; (b) inspection paradox for M/M/1 queues (c) M/G/1 queue with server breakdown Many new examples and exercises.",
            "year": 1975,
            "authors": [
                {
                    "authorId": "2352366",
                    "name": "S. Ross"
                }
            ]
        },
        []
    ],
    "a6b80923dd30f920234a1ac965bea768062882f8": [
        {
            "paperId": "a6b80923dd30f920234a1ac965bea768062882f8",
            "url": "https://www.semanticscholar.org/paper/a6b80923dd30f920234a1ac965bea768062882f8",
            "title": "Efficient Structure Learning of Markov Networks using L1-Regularization",
            "abstract": "Markov networks are commonly used in a wide variety of applications, ranging from computer vision, to natural language, to computational biology. In most current applications, even those that rely heavily on learned models, the structure of the Markov network is constructed by hand, due to the lack of effective algorithms for learning Markov network structure from data. In this paper, we provide a computationally efficient method for learning Markov network structure from data. Our method is based on the use of L1 regularization on the weights of the log-linear model, which has the effect of biasing the model towards solutions wheremany of the parameters are zero. This formulation converts theMarkov network learning problem into a convex optimization problem in a continuous space, which can be solved using efficient gradient methods. A key issue in this setting is the (unavoidable) use of approximate inference, which can lead to errors in the gradient computation when the network structure is dense. Thus, we explore the use of different feature introduction schemes and compare their performance. We provide results for our method on synthetic data, and on two real world data sets: pixel values in the MNIST data, and genetic sequence variations in the human HapMap data. We show that our L1-based method achieves considerably higher generalization performance than the more standard L2-based method (a Gaussian parameter prior) or pure maximum-likelihood learning. We also show that we can learn MRF network structure at a computational cost that is not much greater than learning parameters alone, demonstrating the existence of a feasible method for this important problem.",
            "year": 2007,
            "authors": [
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "145039030",
                    "name": "J. Platt"
                },
                {
                    "authorId": "153379696",
                    "name": "T. Hofmann"
                }
            ]
        },
        []
    ],
    "83644e8e95e1a2034b32d363bc6865bc2e23f8c5": [
        {
            "paperId": "83644e8e95e1a2034b32d363bc6865bc2e23f8c5",
            "url": "https://www.semanticscholar.org/paper/83644e8e95e1a2034b32d363bc6865bc2e23f8c5",
            "title": "Learning in Markov Random Fields An Empirical Study",
            "abstract": "Learning the parameters of an undirected graphical model is particularly difficult due to the presence of a global normalization constant. For large unstructured models computing the gradient of the log-likelihood is intractable and approximations become necessary. Several approximate learning algorithms have been proposed in the literature but a thorough comparative study seems to be absent. In this paper we report on the results of a series of experiments which compare a number of learning algorithms on several models. In our experimental design we use perfect sampling techniques in order to be able to assess quantities such as (asymptotic) normality, bias and variance of the estimates. We envision this effort as a first step towards a more comprehensive open source testing environment where researchers can submit learning algorithms and benchmark problems.",
            "year": 2005,
            "authors": [
                {
                    "authorId": "39530238",
                    "name": "S. Parise"
                },
                {
                    "authorId": "1678311",
                    "name": "M. Welling"
                }
            ]
        },
        []
    ],
    "a5b40921ae03cc6fa34f532173a23c321c9ad3f7": [
        {
            "paperId": "a5b40921ae03cc6fa34f532173a23c321c9ad3f7",
            "url": "https://www.semanticscholar.org/paper/a5b40921ae03cc6fa34f532173a23c321c9ad3f7",
            "title": "The Practical Implementation of Bayesian Model Selection",
            "abstract": "In principle, the Bayesian approach to model selection is straightforward. Prior probability distributions are used to describe the uncertainty surrounding all unknowns. After observing the data, the posterior distribution provides a coherent post data summary of the remaining uncertainty which is relevant for model selection. However, the practical implementation of this approach often requires carefully tailored priors and novel posterior calculation methods. In this article, we illustrate some of the fundamental practical issues that arise for two different model selection problems: the variable selection problem for the linear model and the CART model selection problem. Hugh Chipman is Associate Professor of Statistics, Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, ON N2L 3G1, Canada; email: hachipman@uwaterloo.ca. Edward I. George is Professor of Statistics, Department of Statistics, The Wharton School of the University of Pennsylvania, 3620 Locust Walk, Philadelphia, PA 19104-6302, U.S.A; email: edgeorge@wharton.upenn.edu. Robert E. McCulloch is Professor of Statistics, Graduate School of Business, University of Chicago, 1101 East 58th Street, Chicago, IL, U.S.A; email: Robert.McCulloch@gsb.uchicago.edu. This work was supported by NSF grant DMS-98.03756 and Texas ARP grant 003658.690.",
            "year": 2001,
            "authors": [
                {
                    "authorId": "1960946",
                    "name": "H. Chipman"
                },
                {
                    "authorId": "2347035",
                    "name": "E. George"
                },
                {
                    "authorId": "2840298",
                    "name": "R. McCulloch"
                }
            ]
        },
        [
            "2d1e0a5d5309354d4948fa88b01232e59eee94dc"
        ]
    ],
    "2b461250c014b460e7c97b6138a3ee811f198f43": [
        {
            "paperId": "2b461250c014b460e7c97b6138a3ee811f198f43",
            "url": "https://www.semanticscholar.org/paper/2b461250c014b460e7c97b6138a3ee811f198f43",
            "title": "Generalized Linear Models",
            "abstract": "This is the \u008e rst book on generalized linear models written by authors not mostly associated with the biological sciences. Subtitled \u201cWith Applications in Engineering and the Sciences,\u201d this book\u2019s authors all specialize primarily in engineering statistics. The \u008e rst author has produced several recent editions of Walpole, Myers, and Myers (1998), the last reported by Ziegel (1999). The second author has had several editions of Montgomery and Runger (1999), recently reported by Ziegel (2002). All of the authors are renowned experts in modeling. The \u008e rst two authors collaborated on a seminal volume in applied modeling (Myers and Montgomery 2002), which had its recent revised edition reported by Ziegel (2002). The last two authors collaborated on the most recent edition of a book on regression analysis (Montgomery, Peck, and Vining (2001), reported by Gray (2002), and the \u008e rst author has had multiple editions of his own regression analysis book (Myers 1990), the latest of which was reported by Ziegel (1991). A comparable book with similar objectives and a more speci\u008e c focus on logistic regression, Hosmer and Lemeshow (2000), reported by Conklin (2002), presumed a background in regression analysis and began with generalized linear models. The Preface here (p. xi) indicates an identical requirement but nonetheless begins with 100 pages of material on linear and nonlinear regression. Most of this will probably be a review for the readers of the book. Chapter 2, \u201cLinear Regression Model,\u201d begins with 50 pages of familiar material on estimation, inference, and diagnostic checking for multiple regression. The approach is very traditional, including the use of formal hypothesis tests. In industrial settings, use of p values as part of a risk-weighted decision is generally more appropriate. The pedagologic approach includes formulas and demonstrations for computations, although computing by Minitab is eventually illustrated. Less-familiar material on maximum likelihood estimation, scaled residuals, and weighted least squares provides more speci\u008e c background for subsequent estimation methods for generalized linear models. This review is not meant to be disparaging. The authors have packed a wealth of useful nuggets for any practitioner in this chapter. It is thoroughly enjoyable to read. Chapter 3, \u201cNonlinear Regression Models,\u201d is arguably less of a review, because regression analysis courses often give short shrift to nonlinear models. The chapter begins with a great example on the pitfalls of linearizing a nonlinear model for parameter estimation. It continues with the effective balancing of explicit statements concerning the theoretical basis for computations versus the application and demonstration of their use. The details of maximum likelihood estimation are again provided, and weighted and generalized regression estimation are discussed. Chapter 4 is titled \u201cLogistic and Poisson Regression Models.\u201d Logistic regression provides the basic model for generalized linear models. The prior development for weighted regression is used to motivate maximum likelihood estimation for the parameters in the logistic model. The algebraic details are provided. As in the development for linear models, some of the details are pushed into an appendix. In addition to connecting to the foregoing material on regression on several occasions, the authors link their development forward to their following chapter on the entire family of generalized linear models. They discuss score functions, the variance-covariance matrix, Wald inference, likelihood inference, deviance, and overdispersion. Careful explanations are given for the values provided in standard computer software, here PROC LOGISTIC in SAS. The value in having the book begin with familiar regression concepts is clearly realized when the analogies are drawn between overdispersion and nonhomogenous variance, or analysis of deviance and analysis of variance. The authors rely on the similarity of Poisson regression methods to logistic regression methods and mostly present illustrations for Poisson regression. These use PROC GENMOD in SAS. The book does not give any of the SAS code that produces the results. Two of the examples illustrate designed experiments and modeling. They include discussion of subset selection and adjustment for overdispersion. The mathematic level of the presentation is elevated in Chapter 5, \u201cThe Family of Generalized Linear Models.\u201d First, the authors unify the two preceding chapters under the exponential distribution. The material on the formal structure for generalized linear models (GLMs), likelihood equations, quasilikelihood, the gamma distribution family, and power functions as links is some of the most advanced material in the book. Most of the computational details are relegated to appendixes. A discussion of residuals returns one to a more practical perspective, and two long examples on gamma distribution applications provide excellent guidance on how to put this material into practice. One example is a contrast to the use of linear regression with a log transformation of the response, and the other is a comparison to the use of a different link function in the previous chapter. Chapter 6 considers generalized estimating equations (GEEs) for longitudinal and analogous studies. The \u008e rst half of the chapter presents the methodology, and the second half demonstrates its application through \u008e ve different examples. The basis for the general situation is \u008e rst established using the case with a normal distribution for the response and an identity link. The importance of the correlation structure is explained, the iterative estimation procedure is shown, and estimation for the scale parameters and the standard errors of the coef\u008e cients is discussed. The procedures are then generalized for the exponential family of distributions and quasi-likelihood estimation. Two of the examples are standard repeated-measures illustrations from biostatistical applications, but the last three illustrations are all interesting reworkings of industrial applications. The GEE computations in PROC GENMOD are applied to account for correlations that occur with multiple measurements on the subjects or restrictions to randomizations. The examples show that accounting for correlation structure can result in different conclusions. Chapter 7, \u201cFurther Advances and Applications in GLM,\u201d discusses several additional topics. These are experimental designs for GLMs, asymptotic results, analysis of screening experiments, data transformation, modeling for both a process mean and variance, and generalized additive models. The material on experimental designs is more discursive than prescriptive and as a result is also somewhat theoretical. Similar comments apply for the discussion on the quality of the asymptotic results, which wallows a little too much in reports on various simulation studies. The examples on screening and data transformations experiments are again reworkings of analyses of familiar industrial examples and another obvious motivation for the enthusiasm that the authors have developed for using the GLM toolkit. One can hope that subsequent editions will similarly contain new examples that will have caused the authors to expand the material on generalized additive models and other topics in this chapter. Designating myself to review a book that I know I will love to read is one of the rewards of being editor. I read both of the editions of McCullagh and Nelder (1989), which was reviewed by Schuenemeyer (1992). That book was not fun to read. The obvious enthusiasm of Myers, Montgomery, and Vining and their reliance on their many examples as a major focus of their pedagogy make Generalized Linear Models a joy to read. Every statistician working in any area of applied science should buy it and experience the excitement of these new approaches to familiar activities.",
            "year": 2002,
            "authors": [
                {
                    "authorId": "3444394",
                    "name": "E. Ziegel"
                }
            ]
        },
        []
    ]
}