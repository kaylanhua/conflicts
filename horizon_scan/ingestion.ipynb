{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paper ingestion\n",
    "given a link, I want to ingest the title, abstract, doi, and the doi of the other papers it cites (as well as the papers those papers cite). how many levels down i go in the graph will depend on how stringent my filtering function is.  \n",
    "\n",
    "currently using [this](https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_paper_bulk_search) api from semantic scholar and storing data locally as a json. in the futuer, will aim to use sqlite for persistance instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_doi = '10.1371/journal.pone.0286404'\n",
    "test_paper_id = 'c951ac9a54bba70c6e8337ab5815f3ac45434ec5'\n",
    "\n",
    "def create_url(id, is_doi=False, check_citations=False):\n",
    "    endpoint = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "    fields = '?fields=title,abstract,url,year,authors,references'\n",
    "    if check_citations:\n",
    "        fields += ',citations'\n",
    "    if is_doi:\n",
    "        return endpoint + \"DOI:\" + id + fields\n",
    "    else: \n",
    "        return endpoint + id + fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# calling the semantic scholar api\n",
    "def api_call(api_endpoint):\n",
    "    response = requests.get(api_endpoint)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(api_endpoint)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# a lot of error handling, hasn't broken so far\n",
    "def try_api_call(api_endpoint):\n",
    "    try:\n",
    "        response = requests.get(api_endpoint)\n",
    "        if response.status_code == 429:\n",
    "            print(\"Rate limit reached. Waiting before retrying...\")\n",
    "            time.sleep(20)  # Sleep for a minute, or an appropriate back-off time for your use case\n",
    "            return api_call(api_endpoint)  # Retry the request\n",
    "        response.raise_for_status()  # Will raise an HTTPError for other bad status codes\n",
    "        return response.json()\n",
    "        ## code to keep only the first 3 authors\n",
    "        # if 'authors' in data and len(data['authors']) > 3:\n",
    "        #         data['authors'] = data['authors'][:3]  \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err} - {api_endpoint}\")\n",
    "        return None\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err} - {api_endpoint}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# pretty print json object\n",
    "def pprint(data):\n",
    "    print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords\n",
    "ml_keywords=[\n",
    "    'machine learning', \n",
    "    'regression', \n",
    "    'neural network', \n",
    "    'model',\n",
    "    'projecting',\n",
    "    'forecast',\n",
    "    'predict',\n",
    "    'modeling',\n",
    "    'classification', \n",
    "    'clustering', \n",
    "    'support vector machine', \n",
    "    'decision tree', \n",
    "    'random forest', \n",
    "    'learning', \n",
    "    'gradient boosting', \n",
    "    'data mining', \n",
    "    'natural language processing', \n",
    "    'computer vision', \n",
    "    'algorithm', \n",
    "    'optimization', \n",
    "]\n",
    "conflict_keywords=[\n",
    "    'armed', \n",
    "    'civil',\n",
    "    'battle', \n",
    "    'conflict',\n",
    "    'warfare',\n",
    "    'casualties',\n",
    "    'combat',\n",
    "    'violen',\n",
    "    # 'insurgency', \n",
    "    # 'terrorism', \n",
    "    # 'extremism', \n",
    "    # 'revolution', \n",
    "    # 'violence', \n",
    "    # 'militia', \n",
    "    # 'security', \n",
    "    # 'peacekeeping', \n",
    "    # 'genocide', \n",
    "    # 'massacre', \n",
    "    # 'ceasefire', \n",
    "    # 'rebellion', \n",
    "    # 'humanitarian', \n",
    "    # 'occupation', \n",
    "]\n",
    "all_keywords = ml_keywords + conflict_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_check(string):\n",
    "    string = string.lower()\n",
    "    return any(keyword in string for keyword in all_keywords)\n",
    "\n",
    "## used to create v0 of the graph\n",
    "## generally checks for machine learning + conflicts related keywords (both must be present)\n",
    "def double_keyword_check(string):\n",
    "    string = string.lower()\n",
    "    match1 = any(keyword.lower() in string for keyword in ml_keywords)\n",
    "    match2 = any(keyword.lower() in string for keyword in conflict_keywords)\n",
    "    return match1 and match2\n",
    "\n",
    "def paper_is_relevant(json_blob):\n",
    "    title = json_blob['title']\n",
    "    if double_keyword_check(title):\n",
    "        return True\n",
    "    else:\n",
    "        abstract = json_blob['abstract']\n",
    "        return double_keyword_check(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"database\": will make this a sqlite later\n",
    "# key: paperId, value: [api call data object, how many times hit]\n",
    "papers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_refs(paper_refs):\n",
    "    # first pass at filtering out references that don't pass the general keyword check\n",
    "    filtered_references = [\n",
    "        ref for ref in paper_refs\n",
    "        if keyword_check(ref['title'])\n",
    "    ]\n",
    "    return filtered_references\n",
    "\n",
    "# pprint(filter_refs(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing all the helpers thus far\n",
    "    \n",
    "def add_to_dict(res):\n",
    "    if res:\n",
    "        # there was a response\n",
    "        id = res['paperId']\n",
    "        if id in papers:\n",
    "            print('paper already in database')\n",
    "        else:\n",
    "            refs = filter_refs(res.pop('references', None))\n",
    "            papers[id] = [res, refs, 1]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_a_node(key):\n",
    "    value = papers[key]\n",
    "    # iterate through the refs\n",
    "    for ref in value[1]:\n",
    "        ref_id = ref['paperId']\n",
    "        if ref_id in papers:\n",
    "            # if the ref is already in the database, increment the hit count\n",
    "            papers[ref_id][2] += 1\n",
    "        else:  \n",
    "            # if the ref is not in the database, add it\n",
    "            url = create_url(ref_id)\n",
    "            res = api_call(url)\n",
    "            if res['abstract'] and double_keyword_check(res['abstract']):\n",
    "                add_to_dict(res)\n",
    "    # touched again\n",
    "    papers[key][2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c951ac9a54bba70c6e8337ab5815f3ac45434ec5': [{'paperId': 'c951ac9a54bba70c6e8337ab5815f3ac45434ec5', 'url': 'https://www.semanticscholar.org/paper/c951ac9a54bba70c6e8337ab5815f3ac45434ec5', 'title': 'Modeling analysis of armed conflict risk in sub-Saharan Africa, 2000–2019', 'abstract': 'Sub-Saharan Africa has suffered frequent outbreaks of armed conflict since the end of the Cold War. Although several efforts have been made to understand the underlying causes of armed conflict and establish an early warning mechanism, there is still a lack of a comprehensive assessment approach to model the incidence risk of armed conflict well. Based on a large database of armed conflict events and related spatial datasets covering the period 2000–2019, this study uses a boosted regression tree (BRT) approach to model the spatiotemporal distribution of armed conflict risk in sub-Saharan Africa. Evaluation of accuracy indicates that the simulated models obtain high performance with an area under the receiver operator characteristic curve (ROC-AUC) mean value of 0.937 and an area under the precision recall curves (PR-AUC) mean value of 0.891. The result of the relative contribution indicates that the background context factors (i.e., social welfare and the political system) are the main driving factors of armed conflict risk, with a mean relative contribution of 92.599%. By comparison, the climate change-related variables have relatively little effect on armed conflict risk, accounting for only 7.401% of the total. These results provide novel insight into modelling the incidence risk of armed conflict, which may help implement interventions to prevent and minimize the harm of armed conflict.', 'year': 2023, 'authors': [{'authorId': '2111366015', 'name': 'Xiaolan Xie'}, {'authorId': '2004912912', 'name': 'Dong Jiang'}, {'authorId': '145381688', 'name': 'Mengmeng Hao'}, {'authorId': '11258201', 'name': 'Fangyu Ding'}]}, [{'paperId': '7272b4fa037893b8d54cedd05903360528999557', 'title': 'Varying climatic-social-geographical patterns shape the conflict risk at regional and global scales'}, {'paperId': '5e2092e569a49b8404e4646eb7bfc8df51bc2fba', 'title': 'Lessons from an escalation prediction competition'}, {'paperId': '1ee89d8c4546a34238d715c073d248617d0f67ff', 'title': 'Quantifying the influence of climate variability on armed conflict in Africa, 2000–2015'}, {'paperId': '5a788a70a6b6cb35c6eaf3f335b4476c48d00b69', 'title': 'Modelling armed conflict risk under climate change with machine learning and time-series data'}, {'paperId': '01028e69dfd57d2738dcb39c057bbf952dbcb0af', 'title': 'Projecting armed conflict risk in Africa towards 2050 along the SSP-RCP scenarios: a machine learning approach'}, {'paperId': 'b8974a3094f93e59507c3b775822f3e819ead4db', 'title': 'ViEWS2020: Revising and evaluating the ViEWS political Violence Early-Warning System'}, {'paperId': '43b20758693fbd4df13f4f089d2000f3c837c7b1', 'title': 'Spatiotemporal patterns and spatial risk factors for visceral leishmaniasis from 2007 to 2017 in Western and Central China: A modelling analysis.'}, {'paperId': 'df55a75026085154d0a7cbdf794999939d372b3c', 'title': 'Temperature anomalies affect violent conflicts in African and Middle Eastern warm regions'}, {'paperId': '1e52a479cad4fedd32384aa346233203f9d19bbf', 'title': 'Climate has contrasting direct and indirect effects on armed conflicts'}, {'paperId': 'ef7a4b6f19a92290733151ecb789c9617ad7a9cf', 'title': 'Directions for Research on Climate and Conflict'}, {'paperId': 'ec64ff7c43d6742b3d372b13ddfc2db4f0733f5b', 'title': 'COVID-19 and armed conflict'}, {'paperId': '181f0578d940b6a6bb5c8613e149396709c3d4d1', 'title': 'Multi-method evidence for when and how climate-related disasters contribute to armed conflict risk'}, {'paperId': '3c71eef551cc09ca38f2578c76d315592273ab67', 'title': 'Local warming and violent armed conflict in Africa'}, {'paperId': '9a7f9d5697cb2534278676ff533fecb7b710731c', 'title': 'Risk factors and predicted distribution of visceral leishmaniasis in the Xinjiang Uygur Autonomous Region, China, 2005–2015'}, {'paperId': '49297cc1a5ecca63c34ca6c56699e3c6e6671f9e', 'title': 'Climate as a risk factor for armed conflict'}, {'paperId': 'e3239a1afb502fe194c5fda4675662772a829a1d', 'title': 'ViEWS: A political violence early-warning system'}, {'paperId': 'd80654d515d7f42f20f897893c809e782ce366a1', 'title': 'Climate War in the Middle East? Drought, the Syrian Civil War and the State of Climate-Conflict Research'}, {'paperId': 'c1258a812be1ee84be251f6455e38b1bf0169b96', 'title': 'The study of violence and social unrest in Africa: A comparative analysis of three conflict event datasets'}, {'paperId': 'e81e32f375691820ec2be837d4aed4f54d3eb11a', 'title': 'Environmental impacts and causes of conflict in the Horn of Africa: A review'}, {'paperId': '2e128cd02947725de76622dc04435b217fe4d56d', 'title': 'Predicting Conflict'}, {'paperId': 'aeac3f12ec7b2a490f1f67dcb6c13bc58faaed83', 'title': 'The shape of things to come? Expanding the inequality and grievance model for civil war forecasts with event data'}, {'paperId': '16ce39f8fd5e2036a6a347a667b543e4eef598bb', 'title': 'Climate Change and Collective Violence'}, {'paperId': '2a31dd2f126cd187cc5eb8dbbcd97ad42974ffc3', 'title': 'Civil conflict sensitivity to growing-season drought'}, {'paperId': '67d0efb80486ef0e1fd156f9c12329e15a272f20', 'title': 'An empirical comparison of classification algorithms for mortgage default prediction: evidence from a distressed mortgage market'}, {'paperId': '26acad24d7bbaea5abfe66092b11bca450d8b8ac', 'title': 'Square Pegs in Round Holes: Inequalities, Grievances, and Civil War'}, {'paperId': '7ee54020f8b279d0a82afe22b466db95c498be90', 'title': 'Quantifying the Influence of Climate on Human Conflict'}, {'paperId': '792e75d052a6fb31947b0161ded7c99c8f26a634', 'title': 'Predicting Armed Conflict, 2010–2050'}, {'paperId': '52c2bed21c279b3d31806891756c61790b2399ad', 'title': 'Climate variability and conflict risk in East Africa, 1990–2009'}, {'paperId': '3420b2e619566d5501ee705eeb217518aad1cbd9', 'title': 'Greed and grievance in civil war'}, {'paperId': '5c0155a58d7d2cdc502731769c018fda7b478c2f', 'title': 'Climate Change and Violent Conflict'}, {'paperId': '90cc83858b72542db45ed7664301c2b704bb6f5a', 'title': 'Introducing ACLED: An Armed Conflict Location and Event Dataset'}, {'paperId': 'c545ab4283787fe43cb0cf29de6b46e6bf511957', 'title': 'Political Marginalization, Climate Change, and Conflict in African Sahel States'}, {'paperId': '5642a582b9a9a238e0211ca85e796e9238cd6057', 'title': 'A Global Model for Forecasting Political Instability'}, {'paperId': '5254405a0d1000b57f32a2f6642d105e0b196caf', 'title': 'Warming increases the risk of civil war in Africa'}, {'paperId': '6ba9afd9241a5d04e2ce890f0a6f082d167968b8', 'title': 'A working guide to boosted regression trees.'}, {'paperId': '9f1037a0b5d2a47db49b34010a22b4888386b3f7', 'title': 'POLITICAL LIBERALIZATION OR ARMED CONFLICTS? POLITICAL CHANGES IN POST–COLD WAR AFRICA'}, {'paperId': '2435a02d3afb16ba00ebfa9c8f7df10b52cce80c', 'title': 'Sensitivity Analysis of Empirical Results on Civil War Onset'}, {'paperId': '0bdac412114ce931188d79922d0771a9bb5ba454', 'title': 'People vs. Malthus: Population Pressure, Environmental Degradation, and Armed Conflict Revisited'}, {'paperId': '0824070939b0db6741f032fb0cba54eb01b33ce7', 'title': 'Accounting for scale: Measuring geography in quantitative studies of civil war'}, {'paperId': 'f46c2b6b703c30f2c3684f18a205ae5c0497712e', 'title': 'Modeling the Size of Wars: From Billiard Balls to Sandpiles'}, {'paperId': '4ae60a3b82472ffa0011e086f2fa93828ddefb9a', 'title': 'Ethnicity, Insurgency, and Civil War'}, {'paperId': 'd4e1014f74c7f146be645860e66c80b9a35bf413', 'title': 'On the Duration of Civil War'}, {'paperId': '55d1396f6a53db42856cd110ba569e43316dae32', 'title': 'Modeling Social and Geopolitical Disasters as Extreme Events: A Case Study Considering the Complex Dynamics of International Armed Conflicts'}, {'paperId': '62c37991c1df1600fb4291467b9146154b65f4ff', 'title': 'Revisiting democratic civil peace: Electoral regimes and civil conflict'}, {'paperId': 'dff2ecc7375d14d9ced993e1bbe7f7deead9bc78', 'title': 'Political liberalization or armed conflicts? : Political changes in post-cold war Africa'}], 1]}\n"
     ]
    }
   ],
   "source": [
    "papers = {}\n",
    "\n",
    "# add the first node\n",
    "url = create_url(test_paper_id, False)\n",
    "res = api_call(url)\n",
    "add_to_dict(res)\n",
    "\n",
    "# print(papers)\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to papers.json\n"
     ]
    }
   ],
   "source": [
    "# no need to run\n",
    "from_a_node(test_paper_id)\n",
    "\n",
    "# Define the filename where you want to store the data\n",
    "filename = 'papers.json'\n",
    "\n",
    "# Write the dictionary to a file as JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(papers, file, indent=4)\n",
    "\n",
    "print(f\"Dictionary saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph_to_file(graph_data, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(graph_data, file, indent=4, default=str)  # Using default=str to handle non-serializable data\n",
    "        \n",
    "def save_queue_to_file(queue, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        listed = [list(tup) for tup in queue]\n",
    "        json.dump(listed, file, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## defaults to double keyword check (ML + conflicts) as filtering\n",
    "def generate_reference_network(start_paper_id, max_depth=2, filter_func=double_keyword_check, res_name='graph.json', check_citations=False):\n",
    "    \n",
    "    ## setting up all variables\n",
    "    graph = {}  # Initialize an empty graph, key: paper_id, value: paper_blob, list of references\n",
    "    start_blob = try_api_call(create_url(start_paper_id, check_citations=check_citations))\n",
    "    queue = [(start_paper_id, start_blob, 0)]  # Queue of (paper_id, current_depth)\n",
    "    visited = set()  # Set of visited paper IDs to avoid duplicates\n",
    "    save_interval = 1\n",
    "    processed_papers = 0  \n",
    "\n",
    "    while queue:\n",
    "        current_paper_id, paper_info, current_depth = queue.pop(0)\n",
    "        ## retrying failed api calls added to end of queue\n",
    "        if not paper_info:\n",
    "            url = create_url(ref_id, check_citations=check_citations)\n",
    "            paper_info = try_api_call(url)\n",
    "        if paper_info and current_paper_id not in visited and current_depth <= max_depth:\n",
    "            print(f\"Processing paper {current_paper_id} at depth {current_depth}\")\n",
    "            visited.add(current_paper_id)\n",
    "    \n",
    "            # Retrieve references\n",
    "            if 'references' not in paper_info:\n",
    "                continue\n",
    "            references = filter_refs(paper_info.pop('references', None))\n",
    "            print(\"------- the num of references are: \", len(references))\n",
    "            relevant_refs = []\n",
    "            \n",
    "            ## sheer off excess refs\n",
    "            if len(references) > 100:\n",
    "                references = references[:100]\n",
    "            \n",
    "            children = references\n",
    "            \n",
    "            ## retrieve citations, if requested\n",
    "            if check_citations and 'citations' in paper_info:\n",
    "                if 'citations' in paper_info:\n",
    "                    citations = filter_refs(paper_info.pop('citations', None))\n",
    "                    print(\"------- the num of citations are: \", len(citations))\n",
    "                    if len(citations) > 100:\n",
    "                        citations = citations[:100]\n",
    "                    children += citations\n",
    "            \n",
    "            print(\"------- the num of children are: \", len(children))\n",
    "            \n",
    "            ## iterate through references and citations\n",
    "            for ref in children:\n",
    "                ref_id = ref['paperId']\n",
    "                if ref_id:\n",
    "                    url = create_url(ref_id, check_citations=check_citations)\n",
    "                    res = try_api_call(url)\n",
    "                    \n",
    "                    ## recompute \n",
    "                    if not res:\n",
    "                        queue.append((ref_id, None, current_depth + 1))\n",
    "                    elif res and res['abstract'] and filter_func(res['abstract']):\n",
    "                        relevant_refs.append(ref_id) \n",
    "                        queue.append((ref_id, res, current_depth + 1))\n",
    "                    \n",
    "            ## add processed paper to graph\n",
    "            graph[current_paper_id] = (paper_info, relevant_refs, current_depth)\n",
    "            print(\"------- the rel references are: \", relevant_refs)\n",
    "\n",
    "            processed_papers += 1\n",
    "            \n",
    "            # Save the graph every three papers processed\n",
    "            if processed_papers % save_interval == 0:\n",
    "                save_graph_to_file(graph, res_name)\n",
    "                save_queue_to_file(queue, 'queue.json')\n",
    "                print(f\"Saved backup of graph + queue.\")\n",
    "                \n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## took 23 min 40 sec to get to 129 papers\n",
    "## rate is about 5 papers per minute\n",
    "\n",
    "# This will return a graph dictionary with each paper ID as keys and \n",
    "# the tuple (paper_info, lists of relevant references) as values.\n",
    "# reference_network = generate_reference_network(test_paper_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Network\n",
    "Trying this exclusively for papers about RF as a conflict modeling tactic\n",
    "\n",
    "TODO\n",
    "- [x] when api gateway time out happens, append them to the back of the queue\n",
    "- [x] add depth information to the network as part of \n",
    "- [ ] adding functionality to terminate and keep running \n",
    "- [x] clip the authors after more than three authors\n",
    "- [x] if something has too many references or citations, skip it: 25badc676197a70aaf9911865eb03469e402ba57\n",
    "- [ ] add queue to graph\n",
    "- [x] dump queue into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST PAPERS\n",
    "# first node: \n",
    "# 10.1371/journal.pone.0286404\n",
    "\n",
    "rf_keywords = [\n",
    "    'forest',\n",
    "    'tree',\n",
    "    'random',\n",
    "    'decision tree',\n",
    "    'regression trees',\n",
    "    'boost',\n",
    "    'gradient',\n",
    "    'bagging',\n",
    "    'boosting',\n",
    "    'lightgbm',\n",
    "    'cart',\n",
    "    'adaboost',\n",
    "]\n",
    "\n",
    "## generally checks for keywords relating to random forest classification\n",
    "def filter_trees(string):\n",
    "    string = string.lower()\n",
    "    match1 = any(keyword.lower() in string for keyword in rf_keywords)\n",
    "    # for speed\n",
    "    if not match1:\n",
    "        return False\n",
    "    match2 = any(keyword.lower() in string for keyword in conflict_keywords)\n",
    "    return match1 and match2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the random forest tree generation \n",
    "# title: Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data\n",
    "# published: 2016\n",
    "\n",
    "rf_doi = '10.1093/pan/mpv024'\n",
    "res = try_api_call(create_url(rf_doi, True, True))\n",
    "\n",
    "rf_paper_id = '4b6555beef240120bacb699c7d2f7c8e806b5747'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper 4b6555beef240120bacb699c7d2f7c8e806b5747 at depth 0\n",
      "------- the num of references are:  46\n",
      "------- the num of citations are:  141\n",
      "------- the num of children are:  146\n",
      "HTTP error occurred: 504 Server Error: Gateway Timeout for url: https://api.semanticscholar.org/graph/v1/paper/a4d4053fa12ac75164fe2df0b20a4d3883c292c3?fields=title,abstract,url,year,authors,references,citations - https://api.semanticscholar.org/graph/v1/paper/a4d4053fa12ac75164fe2df0b20a4d3883c292c3?fields=title,abstract,url,year,authors,references,citations\n",
      "HTTP error occurred: 504 Server Error: Gateway Timeout for url: https://api.semanticscholar.org/graph/v1/paper/5455087bbbbddae0e76fb85690758aec9fcce69e?fields=title,abstract,url,year,authors,references,citations - https://api.semanticscholar.org/graph/v1/paper/5455087bbbbddae0e76fb85690758aec9fcce69e?fields=title,abstract,url,year,authors,references,citations\n",
      "------- the rel references are:  ['17c7d2aada72fe59c8d282235b26089d526e44ee', '12e64de7bba4ad1467972309c511d92079477e71', 'b76bdc1d978d921d36069d66b80655ac0aff415b', '038c3b0939f17c5f1c6f31ded255f72a014727ed', '23ce1997237735a2acd2bb149bb5ba74c459a70b', 'ad4275b65b5efd84bf44414df9c7325e35fd0e20', '06548af25270dba0a388f081cb2e748c3581d6e1', '3f1ecd5c4c4db5233596e870a2e19b5bc15e435f', '21a102c6051950aa3df388edb6c878dde9ff3a0a', '18c332b0bad296f8722677d2d63092bf02e0c210']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper a4d4053fa12ac75164fe2df0b20a4d3883c292c3 at depth 1\n",
      "------- the num of references are:  12\n",
      "------- the num of citations are:  0\n",
      "------- the num of children are:  12\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 17c7d2aada72fe59c8d282235b26089d526e44ee at depth 1\n",
      "------- the num of references are:  7\n",
      "------- the num of citations are:  2\n",
      "------- the num of children are:  9\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 12e64de7bba4ad1467972309c511d92079477e71 at depth 1\n",
      "------- the num of references are:  17\n",
      "------- the num of citations are:  2\n",
      "------- the num of children are:  19\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 5455087bbbbddae0e76fb85690758aec9fcce69e at depth 1\n",
      "------- the num of references are:  21\n",
      "------- the num of citations are:  21\n",
      "------- the num of children are:  42\n",
      "------- the rel references are:  ['12e64de7bba4ad1467972309c511d92079477e71', 'b76bdc1d978d921d36069d66b80655ac0aff415b', '12e64de7bba4ad1467972309c511d92079477e71', 'b76bdc1d978d921d36069d66b80655ac0aff415b']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper b76bdc1d978d921d36069d66b80655ac0aff415b at depth 1\n",
      "------- the num of references are:  28\n",
      "------- the num of citations are:  4\n",
      "------- the num of children are:  32\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 038c3b0939f17c5f1c6f31ded255f72a014727ed at depth 1\n",
      "------- the num of references are:  47\n",
      "------- the num of citations are:  3\n",
      "------- the num of children are:  50\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 23ce1997237735a2acd2bb149bb5ba74c459a70b at depth 1\n",
      "------- the num of references are:  75\n",
      "------- the num of citations are:  1\n",
      "------- the num of children are:  76\n",
      "HTTP error occurred: 504 Server Error: Gateway Timeout for url: https://api.semanticscholar.org/graph/v1/paper/5642a582b9a9a238e0211ca85e796e9238cd6057?fields=title,abstract,url,year,authors,references,citations - https://api.semanticscholar.org/graph/v1/paper/5642a582b9a9a238e0211ca85e796e9238cd6057?fields=title,abstract,url,year,authors,references,citations\n",
      "------- the rel references are:  ['2e128cd02947725de76622dc04435b217fe4d56d', '7d5815d9f660054e9cfc458de1b4a120acdbf9de', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper ad4275b65b5efd84bf44414df9c7325e35fd0e20 at depth 1\n",
      "------- the num of references are:  31\n",
      "------- the num of citations are:  0\n",
      "------- the num of children are:  31\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 06548af25270dba0a388f081cb2e748c3581d6e1 at depth 1\n",
      "------- the num of references are:  16\n",
      "------- the num of citations are:  15\n",
      "------- the num of children are:  31\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 3f1ecd5c4c4db5233596e870a2e19b5bc15e435f at depth 1\n",
      "------- the num of references are:  25\n",
      "------- the num of citations are:  5\n",
      "------- the num of children are:  30\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747', '4894e3a3ba9778117453f15c17cfbd9c6280cf47']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 21a102c6051950aa3df388edb6c878dde9ff3a0a at depth 1\n",
      "------- the num of references are:  16\n",
      "------- the num of citations are:  20\n",
      "------- the num of children are:  36\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747', '0c8074f52b1ba53674853b89d42d04ab61a28d69']\n",
      "Saved backup of graph + queue.\n",
      "Processing paper 18c332b0bad296f8722677d2d63092bf02e0c210 at depth 1\n",
      "------- the num of references are:  26\n",
      "------- the num of citations are:  14\n",
      "------- the num of children are:  40\n"
     ]
    }
   ],
   "source": [
    "## Actual generation\n",
    "# time: 6 min 20s for 26 papers\n",
    "\n",
    "rf_network = generate_reference_network(rf_paper_id, max_depth=3, filter_func=filter_trees, res_name='rf_graph.json', check_citations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_api_call(create_url(\"10.1007/978-3-319-55708-3_19\", True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
