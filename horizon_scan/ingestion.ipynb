{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paper ingestion\n",
    "given a link, I want to ingest the title, abstract, doi, and the doi of the other papers it cites (as well as the papers those papers cite). how many levels down i go in the graph will depend on how stringent my filtering function is.  \n",
    "\n",
    "currently using [this](https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_paper_bulk_search) api from semantic scholar and storing data locally as a json. in the futuer, will aim to use sqlite for persistance instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_doi = '10.1371/journal.pone.0286404'\n",
    "test_paper_id = 'c951ac9a54bba70c6e8337ab5815f3ac45434ec5'\n",
    "\n",
    "def create_url(id, is_doi=False, check_citations=False):\n",
    "    endpoint = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "    fields = '?fields=title,abstract,url,year,authors,references'\n",
    "    if check_citations:\n",
    "        fields += ',citations'\n",
    "    if is_doi:\n",
    "        return endpoint + \"DOI:\" + id + fields\n",
    "    else: \n",
    "        return endpoint + id + fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# calling the semantic scholar api\n",
    "def api_call(api_endpoint):\n",
    "    response = requests.get(api_endpoint)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(api_endpoint)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# a lot of error handling, hasn't broken so far\n",
    "def try_api_call(api_endpoint):\n",
    "    try:\n",
    "        response = requests.get(api_endpoint)\n",
    "        if response.status_code == 429:\n",
    "            print(\"Rate limit reached. Waiting before retrying...\")\n",
    "            time.sleep(20)  # Sleep for a minute, or an appropriate back-off time for your use case\n",
    "            return api_call(api_endpoint)  # Retry the request\n",
    "        response.raise_for_status()  # Will raise an HTTPError for other bad status codes\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err} - {api_endpoint}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err} - {api_endpoint}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# pretty print json object\n",
    "def pprint(data):\n",
    "    print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords\n",
    "ml_keywords=[\n",
    "    'machine learning', \n",
    "    'regression', \n",
    "    'neural network', \n",
    "    'model',\n",
    "    'projecting',\n",
    "    'forecast',\n",
    "    'predict',\n",
    "    'modeling',\n",
    "    'classification', \n",
    "    'clustering', \n",
    "    'support vector machine', \n",
    "    'decision tree', \n",
    "    'random forest', \n",
    "    'learning', \n",
    "    'gradient boosting', \n",
    "    'data mining', \n",
    "    'natural language processing', \n",
    "    'computer vision', \n",
    "    'algorithm', \n",
    "    'optimization', \n",
    "]\n",
    "conflict_keywords=[\n",
    "    'armed', \n",
    "    'civil',\n",
    "    'war', \n",
    "    'conflict',\n",
    "    'insurgency', \n",
    "    'terrorism', \n",
    "    'extremism', \n",
    "    'revolution', \n",
    "    'violence', \n",
    "    'warfare', \n",
    "    'battle', \n",
    "    'combat', \n",
    "    'militia', \n",
    "    'security', \n",
    "    'peacekeeping', \n",
    "    'genocide', \n",
    "    'massacre', \n",
    "    'ceasefire', \n",
    "    'rebellion', \n",
    "    'humanitarian', \n",
    "    'occupation', \n",
    "]\n",
    "all_keywords = ml_keywords + conflict_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_check(string):\n",
    "    string = string.lower()\n",
    "    return any(keyword in string for keyword in all_keywords)\n",
    "\n",
    "## used to create v0 of the graph\n",
    "## generally checks for machine learning + conflicts related keywords (both must be present)\n",
    "def double_keyword_check(string):\n",
    "    string = string.lower()\n",
    "    match1 = any(keyword.lower() in string for keyword in ml_keywords)\n",
    "    match2 = any(keyword.lower() in string for keyword in conflict_keywords)\n",
    "    return match1 and match2\n",
    "\n",
    "def paper_is_relevant(json_blob):\n",
    "    title = json_blob['title']\n",
    "    if double_keyword_check(title):\n",
    "        return True\n",
    "    else:\n",
    "        abstract = json_blob['abstract']\n",
    "        return double_keyword_check(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"database\": will make this a sqlite later\n",
    "# key: paperId, value: [api call data object, how many times hit]\n",
    "papers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_refs(paper_refs):\n",
    "    # first pass at filtering out references that don't pass the general keyword check\n",
    "    filtered_references = [\n",
    "        ref for ref in paper_refs\n",
    "        if keyword_check(ref['title'])\n",
    "    ]\n",
    "    return filtered_references\n",
    "\n",
    "# pprint(filter_refs(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing all the helpers thus far\n",
    "    \n",
    "def add_to_dict(res):\n",
    "    if res:\n",
    "        # there was a response\n",
    "        id = res['paperId']\n",
    "        if id in papers:\n",
    "            print('paper already in database')\n",
    "        else:\n",
    "            refs = filter_refs(res.pop('references', None))\n",
    "            papers[id] = [res, refs, 1]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_a_node(key):\n",
    "    value = papers[key]\n",
    "    # iterate through the refs\n",
    "    for ref in value[1]:\n",
    "        ref_id = ref['paperId']\n",
    "        if ref_id in papers:\n",
    "            # if the ref is already in the database, increment the hit count\n",
    "            papers[ref_id][2] += 1\n",
    "        else:  \n",
    "            # if the ref is not in the database, add it\n",
    "            url = create_url(ref_id)\n",
    "            res = api_call(url)\n",
    "            if res['abstract'] and double_keyword_check(res['abstract']):\n",
    "                add_to_dict(res)\n",
    "    # touched again\n",
    "    papers[key][2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c951ac9a54bba70c6e8337ab5815f3ac45434ec5': [{'paperId': 'c951ac9a54bba70c6e8337ab5815f3ac45434ec5', 'url': 'https://www.semanticscholar.org/paper/c951ac9a54bba70c6e8337ab5815f3ac45434ec5', 'title': 'Modeling analysis of armed conflict risk in sub-Saharan Africa, 2000–2019', 'abstract': 'Sub-Saharan Africa has suffered frequent outbreaks of armed conflict since the end of the Cold War. Although several efforts have been made to understand the underlying causes of armed conflict and establish an early warning mechanism, there is still a lack of a comprehensive assessment approach to model the incidence risk of armed conflict well. Based on a large database of armed conflict events and related spatial datasets covering the period 2000–2019, this study uses a boosted regression tree (BRT) approach to model the spatiotemporal distribution of armed conflict risk in sub-Saharan Africa. Evaluation of accuracy indicates that the simulated models obtain high performance with an area under the receiver operator characteristic curve (ROC-AUC) mean value of 0.937 and an area under the precision recall curves (PR-AUC) mean value of 0.891. The result of the relative contribution indicates that the background context factors (i.e., social welfare and the political system) are the main driving factors of armed conflict risk, with a mean relative contribution of 92.599%. By comparison, the climate change-related variables have relatively little effect on armed conflict risk, accounting for only 7.401% of the total. These results provide novel insight into modelling the incidence risk of armed conflict, which may help implement interventions to prevent and minimize the harm of armed conflict.', 'year': 2023, 'authors': [{'authorId': '2111366015', 'name': 'Xiaolan Xie'}, {'authorId': '2004912912', 'name': 'Dong Jiang'}, {'authorId': '145381688', 'name': 'Mengmeng Hao'}, {'authorId': '11258201', 'name': 'Fangyu Ding'}]}, [{'paperId': '7272b4fa037893b8d54cedd05903360528999557', 'title': 'Varying climatic-social-geographical patterns shape the conflict risk at regional and global scales'}, {'paperId': '5e2092e569a49b8404e4646eb7bfc8df51bc2fba', 'title': 'Lessons from an escalation prediction competition'}, {'paperId': '1ee89d8c4546a34238d715c073d248617d0f67ff', 'title': 'Quantifying the influence of climate variability on armed conflict in Africa, 2000–2015'}, {'paperId': '5a788a70a6b6cb35c6eaf3f335b4476c48d00b69', 'title': 'Modelling armed conflict risk under climate change with machine learning and time-series data'}, {'paperId': '01028e69dfd57d2738dcb39c057bbf952dbcb0af', 'title': 'Projecting armed conflict risk in Africa towards 2050 along the SSP-RCP scenarios: a machine learning approach'}, {'paperId': 'b8974a3094f93e59507c3b775822f3e819ead4db', 'title': 'ViEWS2020: Revising and evaluating the ViEWS political Violence Early-Warning System'}, {'paperId': '43b20758693fbd4df13f4f089d2000f3c837c7b1', 'title': 'Spatiotemporal patterns and spatial risk factors for visceral leishmaniasis from 2007 to 2017 in Western and Central China: A modelling analysis.'}, {'paperId': 'df55a75026085154d0a7cbdf794999939d372b3c', 'title': 'Temperature anomalies affect violent conflicts in African and Middle Eastern warm regions'}, {'paperId': '1e52a479cad4fedd32384aa346233203f9d19bbf', 'title': 'Climate has contrasting direct and indirect effects on armed conflicts'}, {'paperId': 'ef7a4b6f19a92290733151ecb789c9617ad7a9cf', 'title': 'Directions for Research on Climate and Conflict'}, {'paperId': 'ec64ff7c43d6742b3d372b13ddfc2db4f0733f5b', 'title': 'COVID-19 and armed conflict'}, {'paperId': '181f0578d940b6a6bb5c8613e149396709c3d4d1', 'title': 'Multi-method evidence for when and how climate-related disasters contribute to armed conflict risk'}, {'paperId': '3c71eef551cc09ca38f2578c76d315592273ab67', 'title': 'Local warming and violent armed conflict in Africa'}, {'paperId': '9a7f9d5697cb2534278676ff533fecb7b710731c', 'title': 'Risk factors and predicted distribution of visceral leishmaniasis in the Xinjiang Uygur Autonomous Region, China, 2005–2015'}, {'paperId': '49297cc1a5ecca63c34ca6c56699e3c6e6671f9e', 'title': 'Climate as a risk factor for armed conflict'}, {'paperId': 'e3239a1afb502fe194c5fda4675662772a829a1d', 'title': 'ViEWS: A political violence early-warning system'}, {'paperId': 'd80654d515d7f42f20f897893c809e782ce366a1', 'title': 'Climate War in the Middle East? Drought, the Syrian Civil War and the State of Climate-Conflict Research'}, {'paperId': 'c1258a812be1ee84be251f6455e38b1bf0169b96', 'title': 'The study of violence and social unrest in Africa: A comparative analysis of three conflict event datasets'}, {'paperId': 'e81e32f375691820ec2be837d4aed4f54d3eb11a', 'title': 'Environmental impacts and causes of conflict in the Horn of Africa: A review'}, {'paperId': '2e128cd02947725de76622dc04435b217fe4d56d', 'title': 'Predicting Conflict'}, {'paperId': 'aeac3f12ec7b2a490f1f67dcb6c13bc58faaed83', 'title': 'The shape of things to come? Expanding the inequality and grievance model for civil war forecasts with event data'}, {'paperId': '16ce39f8fd5e2036a6a347a667b543e4eef598bb', 'title': 'Climate Change and Collective Violence'}, {'paperId': '2a31dd2f126cd187cc5eb8dbbcd97ad42974ffc3', 'title': 'Civil conflict sensitivity to growing-season drought'}, {'paperId': '67d0efb80486ef0e1fd156f9c12329e15a272f20', 'title': 'An empirical comparison of classification algorithms for mortgage default prediction: evidence from a distressed mortgage market'}, {'paperId': '26acad24d7bbaea5abfe66092b11bca450d8b8ac', 'title': 'Square Pegs in Round Holes: Inequalities, Grievances, and Civil War'}, {'paperId': '7ee54020f8b279d0a82afe22b466db95c498be90', 'title': 'Quantifying the Influence of Climate on Human Conflict'}, {'paperId': '792e75d052a6fb31947b0161ded7c99c8f26a634', 'title': 'Predicting Armed Conflict, 2010–2050'}, {'paperId': '52c2bed21c279b3d31806891756c61790b2399ad', 'title': 'Climate variability and conflict risk in East Africa, 1990–2009'}, {'paperId': '3420b2e619566d5501ee705eeb217518aad1cbd9', 'title': 'Greed and grievance in civil war'}, {'paperId': '5c0155a58d7d2cdc502731769c018fda7b478c2f', 'title': 'Climate Change and Violent Conflict'}, {'paperId': '90cc83858b72542db45ed7664301c2b704bb6f5a', 'title': 'Introducing ACLED: An Armed Conflict Location and Event Dataset'}, {'paperId': 'c545ab4283787fe43cb0cf29de6b46e6bf511957', 'title': 'Political Marginalization, Climate Change, and Conflict in African Sahel States'}, {'paperId': '5642a582b9a9a238e0211ca85e796e9238cd6057', 'title': 'A Global Model for Forecasting Political Instability'}, {'paperId': '5254405a0d1000b57f32a2f6642d105e0b196caf', 'title': 'Warming increases the risk of civil war in Africa'}, {'paperId': '6ba9afd9241a5d04e2ce890f0a6f082d167968b8', 'title': 'A working guide to boosted regression trees.'}, {'paperId': '9f1037a0b5d2a47db49b34010a22b4888386b3f7', 'title': 'POLITICAL LIBERALIZATION OR ARMED CONFLICTS? POLITICAL CHANGES IN POST–COLD WAR AFRICA'}, {'paperId': '2435a02d3afb16ba00ebfa9c8f7df10b52cce80c', 'title': 'Sensitivity Analysis of Empirical Results on Civil War Onset'}, {'paperId': '0bdac412114ce931188d79922d0771a9bb5ba454', 'title': 'People vs. Malthus: Population Pressure, Environmental Degradation, and Armed Conflict Revisited'}, {'paperId': '0824070939b0db6741f032fb0cba54eb01b33ce7', 'title': 'Accounting for scale: Measuring geography in quantitative studies of civil war'}, {'paperId': 'f46c2b6b703c30f2c3684f18a205ae5c0497712e', 'title': 'Modeling the Size of Wars: From Billiard Balls to Sandpiles'}, {'paperId': '4ae60a3b82472ffa0011e086f2fa93828ddefb9a', 'title': 'Ethnicity, Insurgency, and Civil War'}, {'paperId': 'd4e1014f74c7f146be645860e66c80b9a35bf413', 'title': 'On the Duration of Civil War'}, {'paperId': '55d1396f6a53db42856cd110ba569e43316dae32', 'title': 'Modeling Social and Geopolitical Disasters as Extreme Events: A Case Study Considering the Complex Dynamics of International Armed Conflicts'}, {'paperId': '62c37991c1df1600fb4291467b9146154b65f4ff', 'title': 'Revisiting democratic civil peace: Electoral regimes and civil conflict'}, {'paperId': 'dff2ecc7375d14d9ced993e1bbe7f7deead9bc78', 'title': 'Political liberalization or armed conflicts? : Political changes in post-cold war Africa'}], 1]}\n"
     ]
    }
   ],
   "source": [
    "papers = {}\n",
    "\n",
    "# add the first node\n",
    "url = create_url(test_paper_id, False)\n",
    "res = api_call(url)\n",
    "add_to_dict(res)\n",
    "\n",
    "# print(papers)\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to papers.json\n"
     ]
    }
   ],
   "source": [
    "# no need to run\n",
    "from_a_node(test_paper_id)\n",
    "\n",
    "# Define the filename where you want to store the data\n",
    "filename = 'papers.json'\n",
    "\n",
    "# Write the dictionary to a file as JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(papers, file, indent=4)\n",
    "\n",
    "print(f\"Dictionary saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph_to_file(graph_data, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(graph_data, file, indent=4, default=str)  # Using default=str to handle non-serializable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## defaults to double keyword check (ML + conflicts) as filtering\n",
    "def generate_reference_network(start_paper_id, max_depth=2, filter_func=double_keyword_check, res_name='graph.json', check_citations=False):\n",
    "    graph = {}  # Initialize an empty graph, key: paper_id, value: paper_blob, list of references\n",
    "    start_blob = try_api_call(create_url(start_paper_id, check_citations=check_citations))\n",
    "    queue = [(start_paper_id, start_blob, 0)]  # Queue of (paper_id, current_depth)\n",
    "    visited = set()  # Set of visited paper IDs to avoid duplicates\n",
    "    save_interval = 1\n",
    "    processed_papers = 0  \n",
    "\n",
    "    while queue:\n",
    "        current_paper_id, paper_info, current_depth = queue.pop(0)\n",
    "        if current_paper_id not in visited and current_depth <= max_depth:\n",
    "            print(f\"Processing paper {current_paper_id} at depth {current_depth}\")\n",
    "            visited.add(current_paper_id)\n",
    "    \n",
    "            # Retrieve references and check if the paper meets the criteria\n",
    "            if 'references' not in paper_info:\n",
    "                continue\n",
    "            references = filter_refs(paper_info.pop('references', None))\n",
    "            print(\"------- the num of references are: \", len(references))\n",
    "            relevant_refs = []\n",
    "            children = references\n",
    "            if check_citations and 'citations' in paper_info:\n",
    "                if 'citations' in paper_info:\n",
    "                    citations = filter_refs(paper_info.pop('citations', None))\n",
    "                    print(\"------- the num of citations are: \", len(citations))\n",
    "                    children += citations\n",
    "            print(\"------- checking through: \", len(children))\n",
    "            \n",
    "            for ref in children:\n",
    "                ref_id = ref['paperId']\n",
    "                if ref_id:\n",
    "                    url = create_url(ref_id, check_citations=check_citations)\n",
    "                    res = try_api_call(url)\n",
    "                    if res and res['abstract'] and filter_func(res['abstract']):\n",
    "                        relevant_refs.append(ref_id) \n",
    "                        queue.append((ref_id, res, current_depth + 1))\n",
    "            \n",
    "            graph[current_paper_id] = (paper_info, relevant_refs)\n",
    "            print(\"------- the rel references are: \", relevant_refs)\n",
    "\n",
    "            processed_papers += 1\n",
    "            \n",
    "            # Save the graph every three papers processed\n",
    "            if processed_papers % save_interval == 0:\n",
    "                save_graph_to_file(graph, res_name)\n",
    "                print(f\"Saved backup of graph at {processed_papers} papers.\")\n",
    "                \n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## took 23 min 40 sec to get to 129 papers\n",
    "## rate is about 5 papers per minute\n",
    "\n",
    "# This will return a graph dictionary with each paper ID as keys and \n",
    "# the tuple (paper_info, lists of relevant references) as values.\n",
    "# reference_network = generate_reference_network(test_paper_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Network\n",
    "Trying this exclusively for papers about RF as a conflict modeling tactic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST PAPERS\n",
    "# first node: \n",
    "# 10.1371/journal.pone.0286404\n",
    "\n",
    "rf_keywords = [\n",
    "    'forest',\n",
    "    'tree',\n",
    "    'random',\n",
    "    'decision tree',\n",
    "    'regression trees',\n",
    "    'boost',\n",
    "    'gradient',\n",
    "    'bagging',\n",
    "    'boosting',\n",
    "    'lightgbm',\n",
    "    'cart',\n",
    "    'adaboost',\n",
    "]\n",
    "\n",
    "## generally checks for keywords relating to random forest classification\n",
    "def filter_trees(string):\n",
    "    string = string.lower()\n",
    "    match1 = any(keyword.lower() in string for keyword in rf_keywords)\n",
    "    # for speed\n",
    "    if not match1:\n",
    "        return False\n",
    "    match2 = any(keyword.lower() in string for keyword in conflict_keywords)\n",
    "    return match1 and match2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running the random forest tree generation \n",
    "# title: Comparing Random Forest with Logistic Regression for Predicting Class-Imbalanced Civil War Onset Data\n",
    "# published: 2016\n",
    "\n",
    "rf_doi = '10.1093/pan/mpv024'\n",
    "res = try_api_call(create_url(rf_doi, True, True))\n",
    "\n",
    "rf_paper_id = '4b6555beef240120bacb699c7d2f7c8e806b5747'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper 4b6555beef240120bacb699c7d2f7c8e806b5747 at depth 0\n",
      "------- the num of references are:  47\n",
      "------- the num of citations are:  146\n",
      "------- checking through:  193\n",
      "------- the rel references are:  ['a4d4053fa12ac75164fe2df0b20a4d3883c292c3', 'da0a744bd257cc50a8778b4452b253bbe3c0b654', '3a8743b858ed557e3c722095ad4d4158e271e0bd', '17c7d2aada72fe59c8d282235b26089d526e44ee', '12e64de7bba4ad1467972309c511d92079477e71', 'b76bdc1d978d921d36069d66b80655ac0aff415b', '0551d31f66171729c1b011584590d411851444b9', '6b393d0fe5210993d17458a082ec9a1bfe040b96', '038c3b0939f17c5f1c6f31ded255f72a014727ed', '97a520228c0d1d5f7410c529a9d924ce187fee77', '23ce1997237735a2acd2bb149bb5ba74c459a70b', 'ad4275b65b5efd84bf44414df9c7325e35fd0e20', '06548af25270dba0a388f081cb2e748c3581d6e1', '3f1ecd5c4c4db5233596e870a2e19b5bc15e435f', '21a102c6051950aa3df388edb6c878dde9ff3a0a', '18c332b0bad296f8722677d2d63092bf02e0c210', 'cf83811d697dc3419a52c9853807afb410eb3943', '8f7892915feb0bd8b16fbe3812b42e6ad6bf05b2', '740d66ba0ab59b1bb38aade6ec7d3170abf1288b', '5f651296106c8860f6a40e831e8f5689f036c56f', '3ac3adc38e2a5fa06e635c4a2a873529ecb981fa', '0e23c5aee6d0c086a2a322fcaec9fad628ae1155', 'fcf53836216c30e43ea8d671f689704679391b78']\n",
      "Saved backup of graph at 1 papers.\n",
      "Processing paper a4d4053fa12ac75164fe2df0b20a4d3883c292c3 at depth 1\n",
      "------- the num of references are:  27\n",
      "------- the num of citations are:  37\n",
      "------- checking through:  64\n",
      "------- the rel references are:  ['18c332b0bad296f8722677d2d63092bf02e0c210', '25badc676197a70aaf9911865eb03469e402ba57', 'b56714e64acbebe7eeffeb2ea665eb7a014ef71b', '84464bbb851210cd5f0e9a3251073398cc70afa4', '18c332b0bad296f8722677d2d63092bf02e0c210', '4e4d8856a01246679bda0d0d00f5208b0eaa1bb0', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 2 papers.\n",
      "Processing paper da0a744bd257cc50a8778b4452b253bbe3c0b654 at depth 1\n",
      "------- the num of references are:  12\n",
      "------- the num of citations are:  8\n",
      "------- checking through:  20\n",
      "------- the rel references are:  ['2d1e0a5d5309354d4948fa88b01232e59eee94dc', '6c00f0abf4f4e0924fee57fad4bd671bc2499f2c', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 3 papers.\n",
      "Processing paper 3a8743b858ed557e3c722095ad4d4158e271e0bd at depth 1\n",
      "------- the num of references are:  48\n",
      "------- the num of citations are:  115\n",
      "------- checking through:  163\n",
      "------- the rel references are:  ['34122df837465c1f2025fabc3209964e7a3a205b', '807c1f19047f96083e13614f7ce20f2ac98c239a', '70c87b478d421a54e2a712861fd23e2e45265062', '06211a9e17bd237ba4769da3031b0c6abd3d1561', '37ae405c2744ba13b88da125d27db4350b79f93d', '85ac963bdb62ad9ef9600dd9699f4c6b517fe8e9', '3f1ecd5c4c4db5233596e870a2e19b5bc15e435f', '6473c0fa09409e9f4b3b8bfdb5956066fa75f8ce', 'b5d53ea5b7444bae5718d6ed37422e321da1bb0f', '18cc0af3d5262eeda57945d755570dcd53bc61a6', 'f3ca01fb5a924e422c014f152e4c266e7dcb85d0', '290563c5234a133cbcc7c497485d50ba7cad35ac', '5e57b70fd51f716f0111355e5e2c24fcb725744e', '3ac3adc38e2a5fa06e635c4a2a873529ecb981fa', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 4 papers.\n",
      "Processing paper 17c7d2aada72fe59c8d282235b26089d526e44ee at depth 1\n",
      "------- the num of references are:  7\n",
      "------- the num of citations are:  2\n",
      "------- checking through:  9\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 5 papers.\n",
      "Processing paper 12e64de7bba4ad1467972309c511d92079477e71 at depth 1\n",
      "------- the num of references are:  17\n",
      "------- the num of citations are:  2\n",
      "------- checking through:  19\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 6 papers.\n",
      "Processing paper b76bdc1d978d921d36069d66b80655ac0aff415b at depth 1\n",
      "------- the num of references are:  28\n",
      "------- the num of citations are:  4\n",
      "------- checking through:  32\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 7 papers.\n",
      "Processing paper 0551d31f66171729c1b011584590d411851444b9 at depth 1\n",
      "------- the num of references are:  64\n",
      "------- the num of citations are:  0\n",
      "------- checking through:  64\n",
      "------- the rel references are:  ['2982952a884a8d27fbcd800c010d8f4af564181b', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 8 papers.\n",
      "Processing paper 6b393d0fe5210993d17458a082ec9a1bfe040b96 at depth 1\n",
      "------- the num of references are:  35\n",
      "------- the num of citations are:  5\n",
      "------- checking through:  40\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 9 papers.\n",
      "Processing paper 038c3b0939f17c5f1c6f31ded255f72a014727ed at depth 1\n",
      "------- the num of references are:  50\n",
      "------- the num of citations are:  3\n",
      "------- checking through:  53\n",
      "------- the rel references are:  ['6dac7ab87bf2f69ba459d95dfae82f499c206847', '43ee50625eb6788a1f6b8819d2a714e869caa0e3', '4b6555beef240120bacb699c7d2f7c8e806b5747', '9925fd0e4b12f36c80bb4240cc098619e3f195d1']\n",
      "Saved backup of graph at 10 papers.\n",
      "Processing paper 97a520228c0d1d5f7410c529a9d924ce187fee77 at depth 1\n",
      "------- the num of references are:  16\n",
      "------- the num of citations are:  3\n",
      "------- checking through:  19\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 11 papers.\n",
      "Processing paper 23ce1997237735a2acd2bb149bb5ba74c459a70b at depth 1\n",
      "------- the num of references are:  78\n",
      "------- the num of citations are:  1\n",
      "------- checking through:  79\n",
      "------- the rel references are:  ['2e128cd02947725de76622dc04435b217fe4d56d', '7d5815d9f660054e9cfc458de1b4a120acdbf9de', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 12 papers.\n",
      "Processing paper ad4275b65b5efd84bf44414df9c7325e35fd0e20 at depth 1\n",
      "------- the num of references are:  50\n",
      "------- the num of citations are:  0\n",
      "------- checking through:  50\n",
      "------- the rel references are:  ['13d9dbab07cf66c1afad32706b9355395a92befb', '6ba9afd9241a5d04e2ce890f0a6f082d167968b8', '38ef724130e211b997df7df172073c500b14e87e', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 13 papers.\n",
      "Processing paper 06548af25270dba0a388f081cb2e748c3581d6e1 at depth 1\n",
      "------- the num of references are:  16\n",
      "------- the num of citations are:  15\n",
      "------- checking through:  31\n",
      "------- the rel references are:  ['3e9a1daf09d089dd365142291ac8e4d68d4afb6b', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 14 papers.\n",
      "Processing paper 3f1ecd5c4c4db5233596e870a2e19b5bc15e435f at depth 1\n",
      "------- the num of references are:  36\n",
      "------- the num of citations are:  6\n",
      "------- checking through:  42\n",
      "------- the rel references are:  ['3a8743b858ed557e3c722095ad4d4158e271e0bd', '4b6555beef240120bacb699c7d2f7c8e806b5747', '4894e3a3ba9778117453f15c17cfbd9c6280cf47']\n",
      "Saved backup of graph at 15 papers.\n",
      "Processing paper 21a102c6051950aa3df388edb6c878dde9ff3a0a at depth 1\n",
      "------- the num of references are:  16\n",
      "------- the num of citations are:  20\n",
      "------- checking through:  36\n",
      "HTTP error occurred: 504 Server Error: Gateway Timeout for url: https://api.semanticscholar.org/graph/v1/paper/ba5afd33db5d86bec96cd0ea4555a2dd09a9b08d?fields=title,abstract,url,year,authors,references,citations - https://api.semanticscholar.org/graph/v1/paper/ba5afd33db5d86bec96cd0ea4555a2dd09a9b08d?fields=title,abstract,url,year,authors,references,citations\n",
      "------- the rel references are:  ['cf83811d697dc3419a52c9853807afb410eb3943', '6ba9afd9241a5d04e2ce890f0a6f082d167968b8', '4b6555beef240120bacb699c7d2f7c8e806b5747', 'e471ea9a00387de0e3db01195ea3175589372c10', '0c8074f52b1ba53674853b89d42d04ab61a28d69', 'ef90c891ff97918171a7035f86df29dd78890950']\n",
      "Saved backup of graph at 16 papers.\n",
      "Processing paper 18c332b0bad296f8722677d2d63092bf02e0c210 at depth 1\n",
      "------- the num of references are:  27\n",
      "------- the num of citations are:  16\n",
      "------- checking through:  43\n",
      "------- the rel references are:  ['a4d4053fa12ac75164fe2df0b20a4d3883c292c3', '873ee91bd87b4ec296db58618c891a39608e7fce', '4b6555beef240120bacb699c7d2f7c8e806b5747', 'a4d4053fa12ac75164fe2df0b20a4d3883c292c3']\n",
      "Saved backup of graph at 17 papers.\n",
      "Processing paper cf83811d697dc3419a52c9853807afb410eb3943 at depth 1\n",
      "------- the num of references are:  23\n",
      "------- the num of citations are:  35\n",
      "------- checking through:  58\n",
      "------- the rel references are:  ['4b6555beef240120bacb699c7d2f7c8e806b5747', '19272846fdfc9445cdc45edb8fcaa072df74caa5', '21a102c6051950aa3df388edb6c878dde9ff3a0a', '3ac3adc38e2a5fa06e635c4a2a873529ecb981fa', 'fcf53836216c30e43ea8d671f689704679391b78']\n",
      "Saved backup of graph at 18 papers.\n",
      "Processing paper 8f7892915feb0bd8b16fbe3812b42e6ad6bf05b2 at depth 1\n",
      "------- the num of references are:  40\n",
      "------- the num of citations are:  14\n",
      "------- checking through:  54\n",
      "------- the rel references are:  ['9e047908e591709b64bba7f2604893fdbf868c3a', '4b6555beef240120bacb699c7d2f7c8e806b5747', '2d82685f7d03fa3d4c1155ef79afdb29bcd88ee0']\n",
      "Saved backup of graph at 19 papers.\n",
      "Processing paper 740d66ba0ab59b1bb38aade6ec7d3170abf1288b at depth 1\n",
      "------- the num of references are:  11\n",
      "------- the num of citations are:  0\n",
      "------- checking through:  11\n",
      "------- the rel references are:  ['bd52ed6a7dfefaee7df6e7d925095503d95bcca1', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 20 papers.\n",
      "Processing paper 5f651296106c8860f6a40e831e8f5689f036c56f at depth 1\n",
      "------- the num of references are:  94\n",
      "------- the num of citations are:  0\n",
      "------- checking through:  94\n",
      "------- the rel references are:  ['5d9547152f1f2c3c8f9389792cab6d251a10a4e5', 'f4cccaf662f24758c80aa6ce731e3b1eb754b536', '936ee66589dbf053fff322b0688c864f0a0c74ec', '7b3c65098b23f1be583db97539e3f163d189a93a', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 21 papers.\n",
      "Processing paper 3ac3adc38e2a5fa06e635c4a2a873529ecb981fa at depth 1\n",
      "------- the num of references are:  86\n",
      "------- the num of citations are:  2\n",
      "------- checking through:  88\n",
      "------- the rel references are:  ['cf83811d697dc3419a52c9853807afb410eb3943', '2f735ee65edd5331bf0f4ceed6677ae789407163', '3a8743b858ed557e3c722095ad4d4158e271e0bd', 'f106cf4f713db3dad98348bd3dec950357189433', 'a6c6244e8e98422bc738fec5f952473df6dc7ebf', '4c1bcea7ef5db705bd931526d855298fbd86b9eb', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 22 papers.\n",
      "Processing paper 0e23c5aee6d0c086a2a322fcaec9fad628ae1155 at depth 1\n",
      "------- the num of references are:  40\n",
      "------- the num of citations are:  0\n",
      "------- checking through:  40\n",
      "------- the rel references are:  ['9e047908e591709b64bba7f2604893fdbf868c3a', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 23 papers.\n",
      "Processing paper fcf53836216c30e43ea8d671f689704679391b78 at depth 1\n",
      "------- the num of references are:  16\n",
      "------- the num of citations are:  0\n",
      "------- checking through:  16\n",
      "------- the rel references are:  ['cf83811d697dc3419a52c9853807afb410eb3943', '25badc676197a70aaf9911865eb03469e402ba57', '4b6555beef240120bacb699c7d2f7c8e806b5747']\n",
      "Saved backup of graph at 24 papers.\n",
      "Processing paper 25badc676197a70aaf9911865eb03469e402ba57 at depth 2\n",
      "------- the num of references are:  535\n",
      "------- the num of citations are:  632\n",
      "------- checking through:  1167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Actual generation\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# time: 6 min 20s for 26 papers\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rf_network \u001b[39m=\u001b[39m generate_reference_network(rf_paper_id, max_depth\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, filter_func\u001b[39m=\u001b[39;49mfilter_trees, res_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrf_graph.json\u001b[39;49m\u001b[39m'\u001b[39;49m, check_citations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m ref_id:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     url \u001b[39m=\u001b[39m create_url(ref_id, check_citations\u001b[39m=\u001b[39mcheck_citations)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     res \u001b[39m=\u001b[39m try_api_call(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mand\u001b[39;00m res[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m filter_func(res[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         relevant_refs\u001b[39m.\u001b[39mappend(ref_id) \n",
      "\u001b[1;32m/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtry_api_call\u001b[39m(api_endpoint):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(api_endpoint)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m429\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaylahuang/Documents/GitHub/conflicts/horizon_scan/ingestion.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRate limit reached. Waiting before retrying...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    792\u001b[0m     conn,\n\u001b[1;32m    793\u001b[0m     method,\n\u001b[1;32m    794\u001b[0m     url,\n\u001b[1;32m    795\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    796\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    797\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    798\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    799\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    800\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    801\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    802\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    803\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    804\u001b[0m )\n\u001b[1;32m    806\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/http/client.py:1411\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1410\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1411\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1412\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1413\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/http/client.py:324\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    325\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/http/client.py:285\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 285\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    287\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/ssl.py:1249\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1246\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1247\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1248\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/c-network/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1106\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Actual generation\n",
    "# time: 6 min 20s for 26 papers\n",
    "\n",
    "rf_network = generate_reference_network(rf_paper_id, max_depth=2, filter_func=filter_trees, res_name='rf_graph.json', check_citations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_api_call(create_url(\"10.1007/978-3-319-55708-3_19\", True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
